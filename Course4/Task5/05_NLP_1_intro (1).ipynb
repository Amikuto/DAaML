{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Классификация текстов при помощи сетей прямого распространения\n",
    "\n",
    "__Автор__: Никита Владимирович Блохин (NVBlokhin@fa.ru)\n",
    "\n",
    "Финансовый университет, 2020 г. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jqDHq_AEjRZ1"
   },
   "source": [
    "## 1. Представление и предобработка текстовых данных "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vaki7efDpmXo"
   },
   "source": [
    "1.1 Операции по предобработке:\n",
    "* токенизация\n",
    "* стемминг / лемматизация\n",
    "* удаление стоп-слов\n",
    "* удаление пунктуации\n",
    "* приведение к нижнему регистру\n",
    "* любые другие операции над текстом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "id": "nHRy4jpYphEr"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "id": "lMMzGhq0ikz1"
   },
   "outputs": [],
   "source": [
    "text = 'Select your preferences and run the install command. Stable represents the most currently tested and supported version of PyTorch. Note that LibTorch is only available for C++'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bUhfertRtXE5"
   },
   "source": [
    "Реализовать функцию `preprocess_text(text: str) -> str`, которая:\n",
    "* приводит строку к нижнему регистру\n",
    "* заменяет все символы, кроме a-z, A-Z и знаков .,!? на пробел\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocess_text(text: str) -> str:\n",
    "    return re.sub(r'[^a-zA-Z.,!?]', ' ', text).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select your preferences and run the install command. stable represents the most currently tested and supported version of pytorch. note that libtorch is only available for c  \n"
     ]
    }
   ],
   "source": [
    "print(preprocess_text(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2Dt1ssIqckC"
   },
   "source": [
    "1.2 Представление текстовых данных при помощи бинарного кодирования\n",
    "\n",
    "\n",
    "Представить первое предложение из `text` в виде тензора `sentence_t`: `sentence_t[i] == 1`, если __слово__ с индексом `i` присуствует в предложении."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['Select', 'your', 'preferences', 'and', 'run', 'the', 'install', 'command']"
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_sen = text.split(\".\")[0]\n",
    "first_sen_list = word_tokenize(first_sen)\n",
    "first_sen_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['Select',\n 'your',\n 'preferences',\n 'and',\n 'run',\n 'the',\n 'install',\n 'command',\n '.']"
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_sentence_list = word_tokenize(sent_tokenize(text)[0])\n",
    "first_sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = set(word_tokenize(text))\n",
    "all_words.remove(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'C++',\n 'LibTorch',\n 'Note',\n 'PyTorch',\n 'Select',\n 'Stable',\n 'and',\n 'available',\n 'command',\n 'currently',\n 'for',\n 'install',\n 'is',\n 'most',\n 'of',\n 'only',\n 'preferences',\n 'represents',\n 'run',\n 'supported',\n 'tested',\n 'that',\n 'the',\n 'version',\n 'your'}"
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1]"
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1 if i in first_sen_list else 0 for i in all_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2Nz_zcgw3N4"
   },
   "source": [
    "## 2. Классификация фамилий по национальности\n",
    "\n",
    "Датасет: https://disk.yandex.ru/d/owHew8hzPc7X9Q?w=1\n",
    "\n",
    "2.1 Считать файл `surnames/surnames.csv`. \n",
    "\n",
    "2.2 Закодировать национальности числами, начиная с 0.\n",
    "\n",
    "2.3 Разбить датасет на обучающую и тестовую выборку\n",
    "\n",
    "2.4 Реализовать класс `Vocab` (токен = __символ__)\n",
    "\n",
    "2.5 Реализовать класс `SurnamesDataset`\n",
    "\n",
    "2.6. Обучить классификатор.\n",
    "\n",
    "2.7 Измерить точность на тестовой выборке. Проверить работоспособность модели: прогнать несколько фамилий студентов группы через модели и проверить результат. Для каждой фамилии выводить 3 наиболее вероятных предсказания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "outputs": [
    {
     "data": {
      "text/plain": "    surname nationality\n0  Woodford     English\n1      Coté      French\n2      Kore     English\n3     Koury      Arabic\n4    Lebzak     Russian",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>surname</th>\n      <th>nationality</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Woodford</td>\n      <td>English</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Coté</td>\n      <td>French</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Kore</td>\n      <td>English</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Koury</td>\n      <td>Arabic</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Lebzak</td>\n      <td>Russian</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 578,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "surname_dataset = pd.read_csv(\"surnames/surnames.csv\")\n",
    "surname_dataset.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "outputs": [
    {
     "data": {
      "text/plain": "{0: 'English',\n 1: 'French',\n 2: 'Arabic',\n 3: 'Russian',\n 4: 'Japanese',\n 5: 'Chinese',\n 6: 'Italian',\n 7: 'Czech',\n 8: 'Irish',\n 9: 'German',\n 10: 'Greek',\n 11: 'Spanish',\n 12: 'Polish',\n 13: 'Dutch',\n 14: 'Vietnamese',\n 15: 'Korean',\n 16: 'Portuguese',\n 17: 'Scottish'}"
     },
     "execution_count": 579,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_nations = surname_dataset.nationality\n",
    "all_nations_dict = pd.Series(all_nations.unique()).to_dict()\n",
    "all_nations_dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "outputs": [
    {
     "data": {
      "text/plain": "{'English': 0,\n 'French': 1,\n 'Arabic': 2,\n 'Russian': 3,\n 'Japanese': 4,\n 'Chinese': 5,\n 'Italian': 6,\n 'Czech': 7,\n 'Irish': 8,\n 'German': 9,\n 'Greek': 10,\n 'Spanish': 11,\n 'Polish': 12,\n 'Dutch': 13,\n 'Vietnamese': 14,\n 'Korean': 15,\n 'Portuguese': 16,\n 'Scottish': 17}"
     },
     "execution_count": 675,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_nations_dict_reverse = {v: k for k, v in all_nations_dict.items()}\n",
    "all_nations_dict_reverse"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "outputs": [
    {
     "data": {
      "text/plain": "10"
     },
     "execution_count": 677,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_nations_dict_reverse[\"Greek\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {
    "id": "kUkSZkDqxNYS"
   },
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "  def __init__(self, data: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Инициализация словаря со всеми символами из данных\n",
    "    :param data: ВЕСЬ датафрейм\n",
    "    \"\"\"\n",
    "    all_chars = pd.Series(pd.unique(data.values.ravel())).map(lambda x: list(x.lower())).explode().unique()\n",
    "    self.idx_to_token = {index: token for index, token in enumerate(all_chars)}\n",
    "    self.token_to_idx = {token: index for index, token in enumerate(all_chars)}\n",
    "    self.vocab_len = len(all_chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 229
    },
    "executionInfo": {
     "elapsed": 1303,
     "status": "error",
     "timestamp": 1619117849212,
     "user": {
      "displayName": "Никита Блохин",
      "photoUrl": "",
      "userId": "16402972581398673009"
     },
     "user_tz": -180
    },
    "id": "WCaRK1QHxe0A",
    "outputId": "5d1243af-d0dd-4922-9468-9618f5df4605",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class SurnamesDataset(Dataset):\n",
    "  def __init__(self, X, y, vocab: Vocab):\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "    self.vocab = vocab\n",
    "    self.max_X = 17\n",
    "    self.max_y = 10\n",
    "\n",
    "  def vectorize(self, surname: str):\n",
    "    '''Генерирует представление фамилии surname в при помощи бинарного кодирования (см. 1.2)'''\n",
    "    tensor = torch.zeros(len(surname), 1, vocab.vocab_len)\n",
    "    for li, letter in enumerate(surname.lower()):\n",
    "      tensor[li][0][vocab.token_to_idx[letter]] = 1\n",
    "    return tensor\n",
    "    # out = []\n",
    "    # for char in surname.lower():\n",
    "    #   out.append(vocab.token_to_idx[char])\n",
    "\n",
    "    # while len(out) < self.max_X:\n",
    "    #   out.insert(0, 0)\n",
    "    # return out\n",
    "\n",
    "      # yield vocab.token_to_idx[char]\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    surname = self.X.iloc[idx]\n",
    "    surname_vectorize = self.vectorize(surname)\n",
    "    # s = torch.tensor(surname_vectorize)\n",
    "    # if len(s) < self.max_X:\n",
    "    #   rp = torch.repeat_interleave(torch.tensor([0]), self.max_X - len(s))\n",
    "    #   s = torch.concat((rp, s), 0)\n",
    "    # else:\n",
    "    #   s = surname_as_tensor\n",
    "\n",
    "    # uniq_nations = self.y.unique()\n",
    "    nation = self.y.iloc[idx]\n",
    "    nation_vectorize = all_nations_dict_reverse[nation]\n",
    "    n = torch.tensor(nation_vectorize)\n",
    "    # if len(n) < self.max_y:\n",
    "    #   rp = torch.repeat_interleave(torch.tensor([0]), self.max_y - len(n))\n",
    "    #   n = torch.concat((rp, n), 0)\n",
    "    # else:\n",
    "    #   n = nation_as_tensor\n",
    "\n",
    "    return surname_vectorize, n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(surname_dataset[\"surname\"], surname_dataset[\"nationality\"], test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "outputs": [
    {
     "data": {
      "text/plain": "(3368       Mckay\n 1680     Yaburov\n 5003        Wang\n 60        Scarsi\n 8657    Charlton\n Name: surname, dtype: object,\n 3368    English\n 1680    Russian\n 5003     German\n 60      Italian\n 8657    English\n Name: nationality, dtype: object)"
     },
     "execution_count": 680,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head(), y_train.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "outputs": [],
   "source": [
    "vocab = Vocab(surname_dataset)\n",
    "snds_train = SurnamesDataset(X=x_train, y=y_train, vocab=vocab)\n",
    "snds_test = SurnamesDataset(X=x_test, y=y_test, vocab=vocab)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "outputs": [
    {
     "data": {
      "text/plain": "{0: 'w',\n 1: 'o',\n 2: 'd',\n 3: 'f',\n 4: 'r',\n 5: 'e',\n 6: 'n',\n 7: 'g',\n 8: 'l',\n 9: 'i',\n 10: 's',\n 11: 'h',\n 12: 'c',\n 13: 't',\n 14: 'é',\n 15: 'k',\n 16: 'u',\n 17: 'y',\n 18: 'a',\n 19: 'b',\n 20: 'z',\n 21: 'j',\n 22: 'p',\n 23: 'm',\n 24: 'v',\n 25: \"'\",\n 26: 'q',\n 27: 'à',\n 28: 'x',\n 29: 'ü',\n 30: '-',\n 31: 'í',\n 32: 'ú',\n 33: 'ä',\n 34: 'ö',\n 35: 'ó',\n 36: '1',\n 37: 'ò',\n 38: 'ñ',\n 39: 'ż',\n 40: 'ß',\n 41: 'á',\n 42: 'è',\n 43: 'ã',\n 44: 'ê',\n 45: 'ì',\n 46: 'ś',\n 47: 'ń',\n 48: 'ù',\n 49: 'ç',\n 50: '/',\n 51: 'õ',\n 52: 'ą',\n 53: 'ł',\n 54: ':'}"
     },
     "execution_count": 682,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.idx_to_token"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "outputs": [],
   "source": [
    "test_surname = \"Kodama\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0.]],\n\n        [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0.]],\n\n        [[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0.]],\n\n        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0.]],\n\n        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0.]],\n\n        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0.]]])"
     },
     "execution_count": 684,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snds_train.vectorize(test_surname)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "outputs": [
    {
     "data": {
      "text/plain": "['k', 'o', 'd', 'a', 'm', 'a']"
     },
     "execution_count": 685,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[snds_train.vocab.idx_to_token[torch.where(snds_train.vectorize(test_surname)[i] == 1)[1][0].item()] for i in range(len(test_surname))]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "outputs": [
    {
     "data": {
      "text/plain": "'Mckay'"
     },
     "execution_count": 686,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snds_train.X.iloc[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "outputs": [],
   "source": [
    "def tensor_to_letter(ten):\n",
    "  return vocab.idx_to_token[torch.where(ten == 1)[1][0].item()]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "outputs": [],
   "source": [
    "def tensor_to_surname(ten):\n",
    "  return [tensor_to_letter(ten[i]) for i in range(ten.shape[0])]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['m', 'c', 'k', 'a', 'y']\n"
     ]
    }
   ],
   "source": [
    "print(tensor_to_surname(snds_train[0][0]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "outputs": [
    {
     "data": {
      "text/plain": "3368       Mckay\n1680     Yaburov\n5003        Wang\n60        Scarsi\n8657    Charlton\n          ...   \n3450     Higuchi\n4674    Cattaneo\n3761      Penzin\n7060      Morcos\n9990       Nakao\nName: surname, Length: 8784, dtype: object"
     },
     "execution_count": 690,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snds_train.X"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([5, 1, 55])"
     },
     "execution_count": 691,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snds_train[0][0].shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "outputs": [],
   "source": [
    "def my_collate(batch):\n",
    "    # data = [item[0] for item in batch]\n",
    "    # target = [item[1] for item in batch]\n",
    "    # print(batch[0])\n",
    "    # target = torch.LongTensor(target)\n",
    "    # print(tensor_to_surname(batch[0][0]))\n",
    "    # print((batch[0][1]))\n",
    "    return batch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 1\n",
    "trainloader = DataLoader(\n",
    "    dataset=snds_train,\n",
    "    # batch_size=batch_size,\n",
    "    # collate_fn=my_collate,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "testloader = DataLoader(\n",
    "    dataset=snds_test,\n",
    "    # batch_size=batch_size,\n",
    "    # collate_fn=my_collate,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([9, 1, 55]), torch.Size([1]))"
     },
     "execution_count": 714,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch, y_batch = next(iter(trainloader))\n",
    "x_batch[0].shape, y_batch.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "outputs": [
    {
     "data": {
      "text/plain": "['l', 'e', 'i', 'b', 'o', 'v', 's', 'k', 'y']"
     },
     "execution_count": 715,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_to_surname(x_batch[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "outputs": [
    {
     "data": {
      "text/plain": "{0: 'English',\n 1: 'French',\n 2: 'Arabic',\n 3: 'Russian',\n 4: 'Japanese',\n 5: 'Chinese',\n 6: 'Italian',\n 7: 'Czech',\n 8: 'Irish',\n 9: 'German',\n 10: 'Greek',\n 11: 'Spanish',\n 12: 'Polish',\n 13: 'Dutch',\n 14: 'Vietnamese',\n 15: 'Korean',\n 16: 'Portuguese',\n 17: 'Scottish'}"
     },
     "execution_count": 717,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_nations_dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "n_hidden = 128\n",
    "n_categories = len(all_nations_dict)\n",
    "rnn = RNN(vocab.vocab_len, n_hidden, n_categories)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "outputs": [],
   "source": [
    "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
    "\n",
    "def train(line_tensor, category_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    # torch.tensor(line_tensor, dtype=torch.long)\n",
    "    for i in range(torch.tensor(line_tensor, dtype=torch.long).size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "\n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "\n",
    "    return output, loss.item()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "outputs": [],
   "source": [
    "# all_categories\n",
    "\n",
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "    return all_nations_dict[category_i], category_i"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Damir\\AppData\\Local\\Temp\\ipykernel_21212\\3948566924.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  for i in range(torch.tensor(line_tensor, dtype=torch.long).size()[0]):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 1.1728065510908756\n",
      "Epoch 1: 1.1589196624866227\n",
      "Epoch 2: 1.1455561319036685\n",
      "Epoch 3: 1.1354402756099449\n",
      "Epoch 4: 1.1268581589133442\n",
      "Epoch 5: 1.1186911872677694\n",
      "Epoch 6: 1.1113091041832905\n",
      "Epoch 7: 1.1047102787004137\n",
      "Epoch 8: 1.0984479405423184\n",
      "Epoch 9: 1.0925621968027757\n",
      "Epoch 10: 1.0875657079992633\n",
      "Epoch 11: 1.0822583346132586\n",
      "Epoch 12: 1.0776309314725394\n",
      "Epoch 13: 1.0734226002741223\n",
      "Epoch 14: 1.0695862895127246\n",
      "Epoch 15: 1.06651692168031\n",
      "Epoch 16: 1.0633078501598971\n",
      "Epoch 17: 1.0603134043829991\n",
      "Epoch 18: 1.0572203702247644\n",
      "Epoch 19: 1.0542386163320954\n"
     ]
    }
   ],
   "source": [
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "n_iters = 1000\n",
    "print_every = 100\n",
    "plot_every = 100\n",
    "\n",
    "for epoch in range(20):\n",
    "    for x, y in trainloader:\n",
    "        output, loss = train(x[0], y)\n",
    "        current_loss += loss\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        correct = '✓' if guess == y else f\"✗ {categoryFromOutput(output[0])[0]}\"\n",
    "        all_losses.append(current_loss)\n",
    "        current_loss = 0\n",
    "    print(f\"Epoch {epoch}: {np.array(all_losses).mean()}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "outputs": [],
   "source": [
    "def evaluate(line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "\n",
    "    return output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> Dovesky\n",
      "(-0.24) Russian\n",
      "(-2.08) Czech\n",
      "(-2.60) English\n",
      "\n",
      "> Jackson\n",
      "(-0.55) English\n",
      "(-1.91) Scottish\n",
      "(-2.12) Russian\n",
      "\n",
      "> Satoshi\n",
      "(-0.79) Japanese\n",
      "(-0.84) Arabic\n",
      "(-3.05) Italian\n",
      "\n",
      "> Kurtaev\n",
      "(-0.08) Russian\n",
      "(-3.91) English\n",
      "(-3.96) Japanese\n",
      "\n",
      "> Turin\n",
      "(-0.83) Russian\n",
      "(-0.95) English\n",
      "(-3.44) Dutch\n",
      "\n",
      "> Kuznetsov\n",
      "(-0.02) Russian\n",
      "(-4.71) Greek\n",
      "(-5.45) Japanese\n"
     ]
    }
   ],
   "source": [
    "def predict(input_line, n_predictions=3):\n",
    "    print('\\n> %s' % input_line)\n",
    "    with torch.no_grad():\n",
    "        output = evaluate(snds_train.vectorize(input_line))\n",
    "\n",
    "        topv, topi = output.topk(n_predictions, 1, True)\n",
    "        predictions = []\n",
    "\n",
    "        for i in range(n_predictions):\n",
    "            value = topv[0][i].item()\n",
    "            category_index = topi[0][i].item()\n",
    "            print('(%.2f) %s' % (value, all_nations_dict[category_index]))\n",
    "            predictions.append([value, all_nations_dict[category_index]])\n",
    "\n",
    "predict('Dovesky')\n",
    "predict('Jackson')\n",
    "predict('Satoshi')\n",
    "predict('Kurtaev')\n",
    "predict('Turin')\n",
    "predict('Kuznetsov')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLmDB3fJtVox"
   },
   "source": [
    "## 3. Классификация обзоров ресторанов\n",
    "\n",
    "Датасет: https://disk.yandex.ru/d/nY1o70JtAuYa8g\n",
    "\n",
    "3.1 Считать файл `yelp/raw_train.csv`. Оставить от исходного датасета 10% строчек.\n",
    "\n",
    "3.2 Воспользоваться функцией `preprocess_text` из 1.1 для обработки текста отзыва. Закодировать рейтинг числами, начиная с 0.\n",
    "\n",
    "3.3 Разбить датасет на обучающую и тестовую выборку\n",
    "\n",
    "3.4 Реализовать класс `Vocab` (токен = слово)\n",
    "\n",
    "3.5 Реализовать класс `ReviewDataset`\n",
    "\n",
    "3.6 Обучить классификатор\n",
    "\n",
    "3.7 Измерить точность на тестовой выборке. Проверить работоспособность модели: придумать небольшой отзыв, прогнать его через модель и вывести номер предсказанного класса (сделать это для явно позитивного и явно негативного отзыва)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_lCTSKZgu68K"
   },
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "  def __init__(self, data):\n",
    "    self.idx_to_token = ...\n",
    "    self.token_to_idx = ...\n",
    "    self.vocab_len = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WXLmCDvcvRmb"
   },
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "  def __init__(self, X, y, vocab: Vocab):\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "    self.vocab = vocab\n",
    "\n",
    "  def vectorize(self, review):\n",
    "    '''Генерирует представление отзыва review при помощи бинарного кодирования (см. 1.2)'''\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return ..."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMJ70DQQj2X/XaG2BMq6jy8",
   "collapsed_sections": [],
   "name": "blank__05_NLP_1_intro.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
