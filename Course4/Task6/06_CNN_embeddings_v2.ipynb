{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://colab.research.google.com/github/Amikuto/DAaML/blob/master/06_CNN_embeddings_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn"
   ],
   "metadata": {
    "id": "rzp4sZEXvqbu"
   },
   "execution_count": 86,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ubw-7tnMq_2O"
   },
   "source": [
    "# 6. Классификация текстов при помощи сверточных сетей\n",
    "\n",
    "__Автор__: Никита Владимирович Блохин (NVBlokhin@fa.ru)\n",
    "\n",
    "Финансовый университет, 2020 г. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tx75RigN8xIJ"
   },
   "source": [
    "## 1. Представление и предобработка текстовых данных в виде последовательностей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LScKIAey9dAM"
   },
   "source": [
    "1.1 Представьте первое предложение из строки `text` как последовательность из индексов слов, входящих в это предложение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "phEw721T9SYW"
   },
   "outputs": [],
   "source": [
    "text = 'Select your preferences and run the install command. Stable represents the most currently tested and supported version of PyTorch. Note that LibTorch is only available for C++'"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "first_sen = nltk.sent_tokenize(text)[0].replace(\".\", \"\").lower()\n",
    "first_sen"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "hyfe6r9msENr",
    "outputId": "22f7b89e-910f-4603-f6f2-9215468a81c8"
   },
   "execution_count": 122,
   "outputs": [
    {
     "data": {
      "text/plain": "'select your preferences and run the install command'"
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "words = nltk.word_tokenize(first_sen)\n",
    "words"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AgvdsqF0rEWi",
    "outputId": "5d57b628-0bae-47cf-b215-8be35ea39eb7"
   },
   "execution_count": 123,
   "outputs": [
    {
     "data": {
      "text/plain": "['select', 'your', 'preferences', 'and', 'run', 'the', 'install', 'command']"
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "words_dict = {k: v for v, k in enumerate(words)}\n",
    "words_dict"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZK1iUyWJrEhC",
    "outputId": "70a5ddaf-27bd-433b-9e47-2fd9fdbab494"
   },
   "execution_count": 124,
   "outputs": [
    {
     "data": {
      "text/plain": "{'select': 0,\n 'your': 1,\n 'preferences': 2,\n 'and': 3,\n 'run': 4,\n 'the': 5,\n 'install': 6,\n 'command': 7}"
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "[words_dict[i] for i in words]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_HYpfPZ2rEoq",
    "outputId": "1afe4e36-a569-4e17-fc5e-0747da414078"
   },
   "execution_count": 125,
   "outputs": [
    {
     "data": {
      "text/plain": "[0, 1, 2, 3, 4, 5, 6, 7]"
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSFQCPtD9x5J"
   },
   "source": [
    "1.2 Представьте первое предложение из строки `text` как последовательность векторов, соответствующих индексам слов. Для представления индекса в виде вектора используйте унитарное кодирование. В результате должен получиться двумерный тензор размера `количество слов в предложении` x `количество уникальных слов`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "RZS4XLV0-buf"
   },
   "outputs": [],
   "source": [
    "text = 'Select your preferences and run the install command. Stable represents the most currently tested and supported version of PyTorch. Note that LibTorch is only available for C++'"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "all_words = nltk.word_tokenize(text.lower().replace(\".\", \"\"))\n",
    "all_words_dict = {k: v for v, k in enumerate(all_words)}\n",
    "all_words_dict"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Da6RwV5Ft8vl",
    "outputId": "11f25514-927b-4698-e883-48d9eb603c09"
   },
   "execution_count": 127,
   "outputs": [
    {
     "data": {
      "text/plain": "{'select': 0,\n 'your': 1,\n 'preferences': 2,\n 'and': 14,\n 'run': 4,\n 'the': 10,\n 'install': 6,\n 'command': 7,\n 'stable': 8,\n 'represents': 9,\n 'most': 11,\n 'currently': 12,\n 'tested': 13,\n 'supported': 15,\n 'version': 16,\n 'of': 17,\n 'pytorch': 18,\n 'note': 19,\n 'that': 20,\n 'libtorch': 21,\n 'is': 22,\n 'only': 23,\n 'available': 24,\n 'for': 25,\n 'c++': 26}"
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "first_sen = nltk.sent_tokenize(text)[0].replace(\".\", \"\").lower()\n",
    "first_sen_words = nltk.word_tokenize(first_sen)\n",
    "first_sen_words"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wLdymMdHt9CM",
    "outputId": "6f6127c8-02c2-4bda-aba3-347d832631a0"
   },
   "execution_count": 128,
   "outputs": [
    {
     "data": {
      "text/plain": "['select', 'your', 'preferences', 'and', 'run', 'the', 'install', 'command']"
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "tensor = torch.zeros(len(first_sen_words), len(all_words_dict))"
   ],
   "metadata": {
    "id": "6XzpTKflt9Wk"
   },
   "execution_count": 129,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for i, word in enumerate(first_sen_words):\n",
    "  tensor[i][all_words_dict[word]] = 1\n",
    "  print(all_words_dict[word])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OiGYhp0Yt9ca",
    "outputId": "8d21b7c9-2d23-4fa8-b467-d75a8a9dbc16"
   },
   "execution_count": 130,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "14\n",
      "4\n",
      "10\n",
      "6\n",
      "7\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "tensor"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XMnPjWx8t9jq",
    "outputId": "4d33702b-bfd9-4c2e-ce53-63fcaf69e79a"
   },
   "execution_count": 131,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0.],\n        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0.]])"
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([8, 25])"
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZvQKHYA-mJN"
   },
   "source": [
    "1.3 Решите задачу 1.2, используя модуль `nn.Embedding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [
    {
     "data": {
      "text/plain": "25"
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocab = Counter(all_words_dict)\n",
    "vocab = sorted(vocab, key=vocab.get, reverse=True)\n",
    "vocab_size = len(vocab)\n",
    "vocab_size"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [
    {
     "data": {
      "text/plain": "{'c++': 0,\n 'for': 1,\n 'available': 2,\n 'only': 3,\n 'is': 4,\n 'libtorch': 5,\n 'that': 6,\n 'note': 7,\n 'pytorch': 8,\n 'of': 9,\n 'version': 10,\n 'supported': 11,\n 'and': 12,\n 'tested': 13,\n 'currently': 14,\n 'most': 15,\n 'the': 16,\n 'represents': 17,\n 'stable': 18,\n 'command': 19,\n 'install': 20,\n 'run': 21,\n 'preferences': 22,\n 'your': 23,\n 'select': 24}"
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx = {word: ind for ind, word in enumerate(vocab)}\n",
    "word2idx"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "outputs": [
    {
     "data": {
      "text/plain": "[24, 23, 22, 12, 21, 16, 20, 19]"
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_sentences = [word2idx[word] for word in first_sen_words]\n",
    "encoded_sentences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.8165,  0.2662, -1.1366, -0.6337, -1.0894,  0.8706,  0.1666,  0.0857,\n          0.8355, -0.3842, -0.2785, -0.2986,  0.8179, -0.0656, -1.2223,  0.7226,\n          1.2288,  0.8772,  1.3329,  1.6523,  0.1787, -0.6863, -0.5514, -1.7908,\n          0.6648],\n        [-1.0429,  1.5159,  0.0171,  0.2410, -0.4140, -1.9278, -0.0953, -0.2972,\n         -0.5538, -0.9271, -2.1029,  0.6576, -0.1910, -1.0597,  0.2231, -0.5538,\n          0.2335,  0.5641,  0.3492,  0.1145, -1.0163, -0.4078,  0.8625, -0.7429,\n          0.7290],\n        [ 0.4876,  0.2572, -0.5709,  1.7909,  0.5164, -1.1272, -1.0606,  0.6148,\n         -1.0395,  0.4794,  0.6611, -0.3650,  0.0302,  1.2100, -0.3174, -0.0062,\n         -0.3299,  2.2702, -0.9089, -0.5905, -0.8196, -0.9299, -0.0580,  0.3652,\n          0.5199],\n        [ 0.1369, -1.5774, -0.5812, -0.3708, -0.4686, -0.7681,  1.4158, -0.0580,\n          0.4623,  0.2257, -0.5860,  0.3584, -0.8206,  0.1281, -0.5581,  2.1217,\n         -2.0848,  2.6783, -0.9423,  0.0971, -0.8848,  0.9052,  1.1320,  2.6096,\n         -1.3887],\n        [-0.0284,  1.4417,  1.2787, -0.7629, -0.1900,  0.3768,  0.5181,  0.5179,\n         -0.2949,  0.6218,  0.1338,  0.0494, -0.3813,  0.0033, -0.2729,  1.3954,\n         -0.3586, -0.4873,  0.7195, -0.0227, -0.8040,  0.5910,  1.1597,  0.6160,\n         -1.2329],\n        [-0.2322, -1.5733, -0.8132,  1.8531,  0.8999, -1.1669, -0.2670,  0.7274,\n         -0.6344, -1.7826,  1.3369, -1.1146,  0.1139, -0.5318, -0.1246,  1.5570,\n         -0.6810,  0.9572,  0.3054,  0.1131, -1.3993,  1.0264,  1.2200, -0.1409,\n         -0.3840],\n        [-1.2191, -1.2332, -1.4313, -1.3193,  0.4511, -0.7190, -1.7314,  0.5197,\n         -1.0520,  1.8243, -0.7596, -0.3354, -0.0911,  0.6231,  0.4759,  0.6607,\n          0.5504, -1.1314,  0.7365, -0.3460,  0.6668,  0.2486, -0.9954,  2.5918,\n          1.8941],\n        [ 2.3528, -1.0128,  0.5088, -0.3393,  1.5392,  0.4219, -0.6021, -0.7438,\n         -0.7759,  0.0221,  0.0848, -0.3332,  0.2019,  0.1355, -1.4656, -0.2453,\n         -1.7042, -1.2220, -0.6080, -2.1801,  0.1400,  0.8562, -0.3562, -0.7536,\n         -0.2571]], grad_fn=<EmbeddingBackward0>)"
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_dim = 25\n",
    "emb_layer = nn.Embedding(vocab_size, emb_dim)\n",
    "word_vectors = emb_layer(torch.LongTensor(encoded_sentences))\n",
    "word_vectors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([8, 25])"
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXjM7qEUNFY_"
   },
   "source": [
    "## 2. Классификация фамилий по национальности (ConvNet)\n",
    "\n",
    "Датасет: https://disk.yandex.ru/d/owHew8hzPc7X9Q?w=1\n",
    "\n",
    "2.1 Считать файл `surnames/surnames.csv`. \n",
    "\n",
    "2.2 Закодировать национальности числами, начиная с 0.\n",
    "\n",
    "2.3 Разбить датасет на обучающую и тестовую выборку\n",
    "\n",
    "2.4 Реализовать класс `Vocab` (токен = __символ__)\n",
    "  * добавьте в словарь специальный токен `<PAD>` с индексом 0\n",
    "  * при создании словаря сохраните длину самой длинной последовательности из набора данных в виде атрибута `max_seq_len`\n",
    "\n",
    "2.5 Реализовать класс `SurnamesDataset`\n",
    "  * метод `__getitem__` возвращает пару: <последовательность индексов токенов (см. 1.1 ), номер класса> \n",
    "  * длина каждой такой последовательности должна быть одинаковой и равной `vocab.max_seq_len`. Чтобы добиться этого, дополните последовательность справа индексом токена `<PAD>` до нужной длины\n",
    "\n",
    "2.6. Обучить классификатор.\n",
    "  \n",
    "  * Для преобразования последовательности индексов в последовательность векторов используйте `nn.Embedding`. Рассмотрите два варианта: \n",
    "    - когда токен представляется в виде унитарного вектора и модуль `nn.Embedding` не обучается\n",
    "    - когда токен представляется в виде вектора небольшой размерности (меньше, чем размер словаря) и модуль `nn.Embedding` обучается\n",
    "\n",
    "  * Используйте одномерные свертки и пулинг (`nn.Conv1d`, `nn.MaxPool1d`)\n",
    "    - обратите внимание, что `nn.Conv1d` ожидает на вход трехмерный тензор размерности `(batch, embedding_dim, seq_len)`\n",
    "\n",
    "2.7 Измерить точность на тестовой выборке. Проверить работоспособность модели: прогнать несколько фамилий студентов группы через модели и проверить результат. Для каждой фамилии выводить 3 наиболее вероятных предсказания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "outputs": [
    {
     "data": {
      "text/plain": "    surname nationality\n0  Woodford     English\n1      Coté      French\n2      Kore     English\n3     Koury      Arabic\n4    Lebzak     Russian",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>surname</th>\n      <th>nationality</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Woodford</td>\n      <td>English</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Coté</td>\n      <td>French</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Kore</td>\n      <td>English</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Koury</td>\n      <td>Arabic</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Lebzak</td>\n      <td>Russian</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surname_dataset = pd.read_csv(\"surnames/surnames.csv\")\n",
    "surname_dataset.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "outputs": [
    {
     "data": {
      "text/plain": "{'English': 0,\n 'French': 1,\n 'Arabic': 2,\n 'Russian': 3,\n 'Japanese': 4,\n 'Chinese': 5,\n 'Italian': 6,\n 'Czech': 7,\n 'Irish': 8,\n 'German': 9,\n 'Greek': 10,\n 'Spanish': 11,\n 'Polish': 12,\n 'Dutch': 13,\n 'Vietnamese': 14,\n 'Korean': 15,\n 'Portuguese': 16,\n 'Scottish': 17}"
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surname_dict = pd.Series(surname_dataset.nationality.unique()).to_dict()\n",
    "surname_dict = dict(map(reversed, surname_dict.items()))\n",
    "surname_dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "outputs": [
    {
     "data": {
      "text/plain": "        surname  nationality\n0      Woodford            0\n1          Coté            1\n2          Kore            0\n3         Koury            2\n4        Lebzak            3\n...         ...          ...\n10975  Quraishi            2\n10976   Innalls            0\n10977      Król           12\n10978    Purvis            0\n10979  Messerli            9\n\n[10980 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>surname</th>\n      <th>nationality</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Woodford</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Coté</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Kore</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Koury</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Lebzak</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>10975</th>\n      <td>Quraishi</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>10976</th>\n      <td>Innalls</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10977</th>\n      <td>Król</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>10978</th>\n      <td>Purvis</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10979</th>\n      <td>Messerli</td>\n      <td>9</td>\n    </tr>\n  </tbody>\n</table>\n<p>10980 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_nation_as_index = surname_dataset.copy()\n",
    "dataset_nation_as_index.nationality = surname_dataset.nationality.map(lambda x: surname_dict[x])\n",
    "dataset_nation_as_index"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset_nation_as_index[\"surname\"], dataset_nation_as_index[\"nationality\"], test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "outputs": [
    {
     "data": {
      "text/plain": "3876          Yamagata\n10923          Breiner\n3663       Likhovskikh\n188             Gassiy\n5228          Rochford\n             ...      \n878           Hasegawa\n3551             Neish\n8284            Abbott\n10202           Keelan\n7947     Abramtchikoff\nName: surname, Length: 8784, dtype: object"
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "outputs": [
    {
     "data": {
      "text/plain": "3876     4\n10923    9\n3663     3\n188      3\n5228     0\n        ..\n878      4\n3551     0\n8284     0\n10202    0\n7947     3\nName: nationality, Length: 8784, dtype: int64"
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "id": "ZGfJX2NP1sw4"
   },
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "  def __init__(self, column: pd.DataFrame | pd.Series):\n",
    "    all_chars = pd.Series(column.values).map(lambda x: list(x.lower())).explode().unique()\n",
    "    all_chars = np.insert(all_chars, 0, \"<PAD>\")\n",
    "    self.idx_to_token = {index: token for index, token in enumerate(all_chars)}\n",
    "    self.token_to_idx = {token: index for index, token in enumerate(all_chars)}\n",
    "    self.max_seq_len = len(all_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "outputs": [
    {
     "data": {
      "text/plain": "56"
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = Vocab(dataset_nation_as_index[\"surname\"])\n",
    "vocab.max_seq_len"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.5 Реализовать класс `SurnamesDataset`\n",
    "  * метод `__getitem__` возвращает пару: <последовательность индексов токенов (см. 1.1 ), номер класса>\n",
    "  * длина каждой такой последовательности должна быть одинаковой и равной `vocab.max_seq_len`. Чтобы добиться этого, дополните последовательность справа индексом токена `<PAD>` до нужной длины"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "id": "GHjCRqQg1sw5"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class SurnamesDataset(Dataset):\n",
    "  def __init__(self, X, y, vocab: Vocab):\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "    self.vocab = vocab\n",
    "    self.max_X = 17\n",
    "    self.max_y = 10\n",
    "\n",
    "  def vectorize(self, surename):\n",
    "    tensor = torch.zeros(vocab.max_seq_len)\n",
    "    for i, val in enumerate(surename):\n",
    "      tensor[i] = vocab.token_to_idx[val.lower()]\n",
    "    tensor[tensor==0] = vocab.token_to_idx[\"<PAD>\"]\n",
    "    return tensor\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    surname = self.X.iloc[idx]\n",
    "\n",
    "    nation = self.y.iloc[idx]\n",
    "\n",
    "    return self.vectorize(surname), nation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "outputs": [],
   "source": [
    "dataset = SurnamesDataset(X=X_train, y=y_train, vocab=vocab)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [
    {
     "data": {
      "text/plain": "('Yamagata', 4)"
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.iloc[0], y_train.iloc[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([12., 16., 20., 16., 24., 16.,  7., 16.,  0.,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n 4)"
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uo-hf5CQ0iWv"
   },
   "source": [
    "## 3. Классификация обзоров на фильмы (ConvNet)\n",
    "\n",
    "Датасет: https://disk.yandex.ru/d/tdinpb0nN_Dsrg\n",
    "\n",
    "2.1 Создайте набор данных на основе файлов polarity/positive_reviews.csv (положительные отзывы) и polarity/negative_reviews.csv (отрицательные отзывы). Разбейте на обучающую и тестовую выборку.\n",
    "  * токен = __слово__\n",
    "  * данные для обучения в датасете представляются в виде последовательности индексов токенов\n",
    "  * словарь создается на основе _только_ обучающей выборки. Для корректной обработки ситуаций, когда в тестовой выборке встретится токен, который не хранится в словаре, добавьте в словарь специальный токен `<UNK>`\n",
    "  * добавьте предобработку текста\n",
    "\n",
    "2.2. Обучите классификатор.\n",
    "  \n",
    "  * Для преобразования последовательности индексов в последовательность векторов используйте `nn.Embedding` \n",
    "    - подберите адекватную размерность вектора эмбеддинга: \n",
    "    - модуль `nn.Embedding` обучается\n",
    "\n",
    "  * Используйте одномерные свертки и пулинг (`nn.Conv1d`, `nn.MaxPool1d`)\n",
    "    - обратите внимание, что `nn.Conv1d` ожидает на вход трехмерный тензор размерности `(batch, embedding_dim, seq_len)`\n",
    "\n",
    "\n",
    "2.7 Измерить точность на тестовой выборке. Проверить работоспособность модели: придумать небольшой отзыв, прогнать его через модель и вывести номер предсказанного класса (сделать это для явно позитивного и явно негативного отзыва)\n",
    "* Целевое значение accuracy на валидации - 70+%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
