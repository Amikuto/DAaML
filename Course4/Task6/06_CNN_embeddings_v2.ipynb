{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://colab.research.google.com/github/Amikuto/DAaML/blob/master/06_CNN_embeddings_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ],
   "metadata": {
    "id": "rzp4sZEXvqbu"
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ubw-7tnMq_2O"
   },
   "source": [
    "# 6. Классификация текстов при помощи сверточных сетей\n",
    "\n",
    "__Автор__: Никита Владимирович Блохин (NVBlokhin@fa.ru)\n",
    "\n",
    "Финансовый университет, 2020 г. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tx75RigN8xIJ"
   },
   "source": [
    "## 1. Представление и предобработка текстовых данных в виде последовательностей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LScKIAey9dAM"
   },
   "source": [
    "1.1 Представьте первое предложение из строки `text` как последовательность из индексов слов, входящих в это предложение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "phEw721T9SYW"
   },
   "outputs": [],
   "source": [
    "text = 'Select your preferences and run the install command. Stable represents the most currently tested and supported version of PyTorch. Note that LibTorch is only available for C++'"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "first_sen = nltk.sent_tokenize(text)[0].replace(\".\", \"\").lower()\n",
    "first_sen"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "hyfe6r9msENr",
    "outputId": "22f7b89e-910f-4603-f6f2-9215468a81c8"
   },
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "'select your preferences and run the install command'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "words = nltk.word_tokenize(first_sen)\n",
    "words"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AgvdsqF0rEWi",
    "outputId": "5d57b628-0bae-47cf-b215-8be35ea39eb7"
   },
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "['select', 'your', 'preferences', 'and', 'run', 'the', 'install', 'command']"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "words_dict = {k: v for v, k in enumerate(words)}\n",
    "words_dict"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZK1iUyWJrEhC",
    "outputId": "70a5ddaf-27bd-433b-9e47-2fd9fdbab494"
   },
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "{'select': 0,\n 'your': 1,\n 'preferences': 2,\n 'and': 3,\n 'run': 4,\n 'the': 5,\n 'install': 6,\n 'command': 7}"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "[words_dict[i] for i in words]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_HYpfPZ2rEoq",
    "outputId": "1afe4e36-a569-4e17-fc5e-0747da414078"
   },
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "[0, 1, 2, 3, 4, 5, 6, 7]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSFQCPtD9x5J"
   },
   "source": [
    "1.2 Представьте первое предложение из строки `text` как последовательность векторов, соответствующих индексам слов. Для представления индекса в виде вектора используйте унитарное кодирование. В результате должен получиться двумерный тензор размера `количество слов в предложении` x `количество уникальных слов`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "RZS4XLV0-buf"
   },
   "outputs": [],
   "source": [
    "text = 'Select your preferences and run the install command. Stable represents the most currently tested and supported version of PyTorch. Note that LibTorch is only available for C++'"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "all_words = nltk.word_tokenize(text.lower().replace(\".\", \"\"))\n",
    "all_words_dict = {k: v for v, k in enumerate(all_words)}\n",
    "all_words_dict"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Da6RwV5Ft8vl",
    "outputId": "11f25514-927b-4698-e883-48d9eb603c09"
   },
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "{'select': 0,\n 'your': 1,\n 'preferences': 2,\n 'and': 14,\n 'run': 4,\n 'the': 10,\n 'install': 6,\n 'command': 7,\n 'stable': 8,\n 'represents': 9,\n 'most': 11,\n 'currently': 12,\n 'tested': 13,\n 'supported': 15,\n 'version': 16,\n 'of': 17,\n 'pytorch': 18,\n 'note': 19,\n 'that': 20,\n 'libtorch': 21,\n 'is': 22,\n 'only': 23,\n 'available': 24,\n 'for': 25,\n 'c++': 26}"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "first_sen = nltk.sent_tokenize(text)[0].replace(\".\", \"\").lower()\n",
    "first_sen_words = nltk.word_tokenize(first_sen)\n",
    "first_sen_words"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wLdymMdHt9CM",
    "outputId": "6f6127c8-02c2-4bda-aba3-347d832631a0"
   },
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "['select', 'your', 'preferences', 'and', 'run', 'the', 'install', 'command']"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "tensor = torch.zeros(len(first_sen_words), len(all_words_dict))"
   ],
   "metadata": {
    "id": "6XzpTKflt9Wk"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for i, word in enumerate(first_sen_words):\n",
    "  tensor[i][all_words_dict[word]] = 1\n",
    "  print(all_words_dict[word])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OiGYhp0Yt9ca",
    "outputId": "8d21b7c9-2d23-4fa8-b467-d75a8a9dbc16"
   },
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "14\n",
      "4\n",
      "10\n",
      "6\n",
      "7\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "tensor"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XMnPjWx8t9jq",
    "outputId": "4d33702b-bfd9-4c2e-ce53-63fcaf69e79a"
   },
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0.],\n        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0.]])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([8, 25])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZvQKHYA-mJN"
   },
   "source": [
    "1.3 Решите задачу 1.2, используя модуль `nn.Embedding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "25"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocab = Counter(all_words_dict)\n",
    "vocab = sorted(vocab, key=vocab.get, reverse=True)\n",
    "vocab_size = len(vocab)\n",
    "vocab_size"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "{'c++': 0,\n 'for': 1,\n 'available': 2,\n 'only': 3,\n 'is': 4,\n 'libtorch': 5,\n 'that': 6,\n 'note': 7,\n 'pytorch': 8,\n 'of': 9,\n 'version': 10,\n 'supported': 11,\n 'and': 12,\n 'tested': 13,\n 'currently': 14,\n 'most': 15,\n 'the': 16,\n 'represents': 17,\n 'stable': 18,\n 'command': 19,\n 'install': 20,\n 'run': 21,\n 'preferences': 22,\n 'your': 23,\n 'select': 24}"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx = {word: ind for ind, word in enumerate(vocab)}\n",
    "word2idx"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "[24, 23, 22, 12, 21, 16, 20, 19]"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_sentences = [word2idx[word] for word in first_sen_words]\n",
    "encoded_sentences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.1483,  2.4187,  1.3279, -0.2639,  0.3645,  2.5440, -2.6895,  2.4426,\n          0.0104, -0.5644, -0.9184, -0.7496, -0.0949,  1.1009,  1.3105, -0.2928,\n          0.4181, -0.1695, -2.1749,  0.7202,  0.2854,  0.2290,  1.2833, -1.3792,\n          1.4042],\n        [-0.5297, -0.8733,  0.0043, -1.2579, -1.0845,  0.7530,  0.3236, -0.2750,\n          1.3056,  0.2118,  0.2720, -0.9268, -2.7330, -0.5642, -0.2740,  0.1398,\n          0.5086,  0.2771, -0.9812,  0.8888,  1.5690, -0.0819, -0.3494,  0.2024,\n         -0.2884],\n        [ 0.0940, -0.2021, -0.0595,  2.0118, -0.3368,  0.3260,  0.5352,  1.9733,\n         -0.2075, -0.0306,  0.1267,  0.0055,  0.7943,  0.4072, -0.3609,  1.3103,\n         -0.9651,  0.8806, -0.1025, -0.6770, -0.4107, -1.6186,  0.5079,  2.3230,\n          0.2298],\n        [ 1.4628, -0.6204,  0.9884, -0.4322, -0.6232, -0.2162, -0.4887,  0.7870,\n          0.1076, -1.0715, -0.1166, -1.0170, -1.1980,  0.4784, -1.2295, -1.3700,\n          1.5435, -0.0332, -0.4186, -0.2556, -0.1292, -0.0546,  0.4083,  1.1264,\n          1.9351],\n        [ 1.2532, -0.4445,  0.8185, -0.8180,  0.3603, -1.6146, -2.4734,  0.0362,\n         -0.3422, -0.3817, -0.0569,  0.8436,  0.6829,  3.3944, -1.6688,  0.5109,\n         -0.2860,  0.3351,  1.1719,  1.2955,  0.8909, -0.4898, -1.1727, -0.6870,\n         -2.3349],\n        [-0.7645,  0.2408,  0.1664, -2.2318,  1.3892, -0.5023,  1.6797, -1.0240,\n          1.6859, -1.2177,  0.7650,  1.1971, -0.7128, -0.0656,  2.2050,  1.7852,\n         -0.0118,  0.9797, -1.0661,  1.7720, -0.2793, -0.2769,  0.7489, -0.6435,\n         -0.9518],\n        [-0.1642, -0.9715, -1.0308,  0.6473, -0.1906,  0.7167, -2.0002, -2.4097,\n          0.2194, -1.6989,  1.3094, -1.6613, -0.5461, -0.6302, -0.6347,  0.9747,\n          0.2098,  0.0299,  1.7092, -0.7258, -0.7735,  0.5962, -1.2504,  1.1456,\n          0.7393],\n        [ 0.0358,  0.2160, -0.9161,  1.5599, -3.1537, -0.5611, -0.4303, -0.3332,\n         -1.5464, -0.0147,  1.2251,  1.5936, -1.6315, -0.0569,  0.6297,  0.2712,\n         -0.6860, -1.0918,  1.6797, -0.8808,  0.5800,  0.3642,  0.0881, -1.3069,\n         -0.7064]], grad_fn=<EmbeddingBackward0>)"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_dim = 25\n",
    "emb_layer = nn.Embedding(vocab_size, emb_dim)\n",
    "word_vectors = emb_layer(torch.LongTensor(encoded_sentences))\n",
    "word_vectors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([8, 25])"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXjM7qEUNFY_"
   },
   "source": [
    "## 2. Классификация фамилий по национальности (ConvNet)\n",
    "\n",
    "Датасет: https://disk.yandex.ru/d/owHew8hzPc7X9Q?w=1\n",
    "\n",
    "2.1 Считать файл `surnames/surnames.csv`. \n",
    "\n",
    "2.2 Закодировать национальности числами, начиная с 0.\n",
    "\n",
    "2.3 Разбить датасет на обучающую и тестовую выборку\n",
    "\n",
    "2.4 Реализовать класс `Vocab` (токен = __символ__)\n",
    "  * добавьте в словарь специальный токен `<PAD>` с индексом 0\n",
    "  * при создании словаря сохраните длину самой длинной последовательности из набора данных в виде атрибута `max_seq_len`\n",
    "\n",
    "2.5 Реализовать класс `SurnamesDataset`\n",
    "  * метод `__getitem__` возвращает пару: <последовательность индексов токенов (см. 1.1 ), номер класса> \n",
    "  * длина каждой такой последовательности должна быть одинаковой и равной `vocab.max_seq_len`. Чтобы добиться этого, дополните последовательность справа индексом токена `<PAD>` до нужной длины\n",
    "\n",
    "2.6. Обучить классификатор.\n",
    "  \n",
    "  * Для преобразования последовательности индексов в последовательность векторов используйте `nn.Embedding`. Рассмотрите два варианта: \n",
    "    - когда токен представляется в виде унитарного вектора и модуль `nn.Embedding` не обучается\n",
    "    - когда токен представляется в виде вектора небольшой размерности (меньше, чем размер словаря) и модуль `nn.Embedding` обучается\n",
    "\n",
    "  * Используйте одномерные свертки и пулинг (`nn.Conv1d`, `nn.MaxPool1d`)\n",
    "    - обратите внимание, что `nn.Conv1d` ожидает на вход трехмерный тензор размерности `(batch, embedding_dim, seq_len)`\n",
    "\n",
    "2.7 Измерить точность на тестовой выборке. Проверить работоспособность модели: прогнать несколько фамилий студентов группы через модели и проверить результат. Для каждой фамилии выводить 3 наиболее вероятных предсказания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "    surname nationality\n0  Woodford     English\n1      Coté      French\n2      Kore     English\n3     Koury      Arabic\n4    Lebzak     Russian",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>surname</th>\n      <th>nationality</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Woodford</td>\n      <td>English</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Coté</td>\n      <td>French</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Kore</td>\n      <td>English</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Koury</td>\n      <td>Arabic</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Lebzak</td>\n      <td>Russian</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surname_dataset = pd.read_csv(\"./surnames/surnames.csv\")\n",
    "surname_dataset.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "{'English': 0,\n 'French': 1,\n 'Arabic': 2,\n 'Russian': 3,\n 'Japanese': 4,\n 'Chinese': 5,\n 'Italian': 6,\n 'Czech': 7,\n 'Irish': 8,\n 'German': 9,\n 'Greek': 10,\n 'Spanish': 11,\n 'Polish': 12,\n 'Dutch': 13,\n 'Vietnamese': 14,\n 'Korean': 15,\n 'Portuguese': 16,\n 'Scottish': 17}"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surname_dict = pd.Series(surname_dataset.nationality.unique()).to_dict()\n",
    "surname_dict = dict(map(reversed, surname_dict.items()))\n",
    "surname_dict_reverse = {v: k for k, v in surname_dict.items()}\n",
    "surname_dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "        surname  nationality\n0      Woodford            0\n1          Coté            1\n2          Kore            0\n3         Koury            2\n4        Lebzak            3\n...         ...          ...\n10975  Quraishi            2\n10976   Innalls            0\n10977      Król           12\n10978    Purvis            0\n10979  Messerli            9\n\n[10980 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>surname</th>\n      <th>nationality</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Woodford</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Coté</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Kore</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Koury</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Lebzak</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>10975</th>\n      <td>Quraishi</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>10976</th>\n      <td>Innalls</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10977</th>\n      <td>Król</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>10978</th>\n      <td>Purvis</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10979</th>\n      <td>Messerli</td>\n      <td>9</td>\n    </tr>\n  </tbody>\n</table>\n<p>10980 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_nation_as_index = surname_dataset.copy()\n",
    "dataset_nation_as_index.nationality = surname_dataset.nationality.map(lambda x: surname_dict[x])\n",
    "dataset_nation_as_index"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset_nation_as_index[\"surname\"], dataset_nation_as_index[\"nationality\"], test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "10033    Birshtein\n9848         Touma\n5897        Sponer\n488        Hamacho\n1920          Togo\n           ...    \n5559        Aiello\n7234       Samuels\n1716     Jatsevich\n6517      Petrakis\n1091      Tunneson\nName: surname, Length: 8784, dtype: object"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "17"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(X_train.map(len))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "10033     3\n9848      2\n5897      7\n488       4\n1920      4\n         ..\n5559      6\n7234      0\n1716      3\n6517     10\n1091     13\nName: nationality, Length: 8784, dtype: int64"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ZGfJX2NP1sw4"
   },
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "  def __init__(self, column: pd.DataFrame | pd.Series):\n",
    "    all_chars = pd.Series(column.values).map(lambda x: list(x.lower())).explode().unique()\n",
    "    all_chars = np.insert(all_chars, 0, \"<PAD>\")\n",
    "    self.idx_to_token = {index: token for index, token in enumerate(all_chars)}\n",
    "    self.token_to_idx = {token: index for index, token in enumerate(all_chars)}\n",
    "    self.max_seq_len = max(column.map(len))\n",
    "    self.vocab_len = len(all_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "17"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = Vocab(dataset_nation_as_index[\"surname\"])\n",
    "vocab.max_seq_len"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "56"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.vocab_len"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "{'<PAD>': 0,\n 'w': 1,\n 'o': 2,\n 'd': 3,\n 'f': 4,\n 'r': 5,\n 'c': 6,\n 't': 7,\n 'é': 8,\n 'k': 9,\n 'e': 10,\n 'u': 11,\n 'y': 12,\n 'l': 13,\n 'b': 14,\n 'z': 15,\n 'a': 16,\n 'i': 17,\n 'n': 18,\n 'h': 19,\n 'm': 20,\n 's': 21,\n 'v': 22,\n 'p': 23,\n 'g': 24,\n 'j': 25,\n \"'\": 26,\n 'q': 27,\n 'à': 28,\n 'x': 29,\n 'ü': 30,\n '-': 31,\n 'í': 32,\n 'ú': 33,\n 'ä': 34,\n 'ö': 35,\n 'ó': 36,\n '1': 37,\n 'ò': 38,\n 'ñ': 39,\n 'ż': 40,\n 'ß': 41,\n 'á': 42,\n 'è': 43,\n 'ã': 44,\n 'ê': 45,\n 'ì': 46,\n 'ś': 47,\n 'ń': 48,\n 'ù': 49,\n 'ç': 50,\n '/': 51,\n 'õ': 52,\n 'ą': 53,\n 'ł': 54,\n ':': 55}"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.token_to_idx"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.5 Реализовать класс `SurnamesDataset`\n",
    "  * метод `__getitem__` возвращает пару: <последовательность индексов токенов (см. 1.1 ), номер класса>\n",
    "  * длина каждой такой последовательности должна быть одинаковой и равной `vocab.max_seq_len`. Чтобы добиться этого, дополните последовательность справа индексом токена `<PAD>` до нужной длины"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "GHjCRqQg1sw5"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class SurnamesDataset(Dataset):\n",
    "  def __init__(self, X, y, vocab: Vocab, embed: bool=False):\n",
    "    self.embed = embed\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "    self.vocab = vocab\n",
    "    self.max_X = vocab.max_seq_len\n",
    "    self.max_y = 10\n",
    "\n",
    "  def vectorize(self, surname):\n",
    "    if not self.embed:\n",
    "      tensor = torch.zeros(self.max_X, vocab.vocab_len)\n",
    "      for li, letter in enumerate(surname.lower()):\n",
    "        tensor[li][vocab.token_to_idx[letter]] = 1\n",
    "      return tensor\n",
    "\n",
    "    tensor = torch.zeros(self.vocab.max_seq_len, dtype=torch.long)\n",
    "    for i, val in enumerate(surname):\n",
    "      tensor[i] = self.vocab.token_to_idx[val.lower()]\n",
    "    tensor[tensor==0] = self.vocab.token_to_idx[\"<PAD>\"]\n",
    "    return tensor\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    surname = self.X.iloc[idx]\n",
    "\n",
    "    nation_tensor = torch.zeros(len(surname_dict))\n",
    "    nation = self.y.iloc[idx]\n",
    "    nation_tensor[nation] = 1\n",
    "\n",
    "    # return emb(torch.LongTensor(self.vectorize(surname))), nation\n",
    "    return self.vectorize(surname), torch.tensor(nation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "('Birshtein', 3)"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.iloc[0], y_train.iloc[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "index_dataset_train = SurnamesDataset(X=X_train, y=y_train, vocab=vocab)\n",
    "index_dataset_test = SurnamesDataset(X=X_test, y=y_test, vocab=vocab)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([17, 56])"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_dataset_train[0][0].shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "hot_dataset_train = SurnamesDataset(X=X_train, y=y_train, vocab=vocab, embed=True)\n",
    "hot_dataset_test = SurnamesDataset(X=X_test, y=y_test, vocab=vocab, embed=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([14, 17,  5, 21, 19,  7, 10, 17, 18,  0,  0,  0,  0,  0,  0,  0,  0])"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hot_dataset_train[0][0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.6. Обучить классификатор.\n",
    "\n",
    "  * Для преобразования последовательности индексов в последовательность векторов используйте `nn.Embedding`. Рассмотрите два варианта:\n",
    "    - когда токен представляется в виде унитарного вектора и модуль `nn.Embedding` не обучается\n",
    "    - когда токен представляется в виде вектора небольшой размерности (меньше, чем размер словаря) и модуль `nn.Embedding` обучается\n",
    "\n",
    "  * Используйте одномерные свертки и пулинг (`nn.Conv1d`, `nn.MaxPool1d`)\n",
    "    - обратите внимание, что `nn.Conv1d` ожидает на вход трехмерный тензор размерности `(batch, embedding_dim, seq_len)`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SurnameClassifier(nn.Module):\n",
    "  \"\"\" A 2-layer multilayer perceptron for classifying surnames \"\"\"\n",
    "  def __init__(self, input_dim, hidden_dim, output_dim, embeded=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        input_dim (int): the size of the input vectors\n",
    "        hidden_dim (int): the output size of the first Linear layer\n",
    "        output_dim (int): the output size of the second Linear layer\n",
    "    \"\"\"\n",
    "    super(SurnameClassifier, self).__init__()\n",
    "    self.embeded = embeded\n",
    "    self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "    self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    self.embedding = nn.Embedding(num_embeddings=len(vocab.alphabet), embedding_dim=64)\n",
    "\n",
    "  def forward(self, x_in, apply_softmax=False):\n",
    "    \"\"\"The forward pass of the classifier\n",
    "\n",
    "    Args:\n",
    "        x_in (torch.Tensor): an input data tensor\n",
    "            x_in.shape should be (batch, input_dim)\n",
    "        apply_softmax (bool): a flag for the softmax activation\n",
    "            should be false if used with the cross-entropy losses\n",
    "    Returns:\n",
    "        the resulting tensor. tensor.shape should be (batch, output_dim).\n",
    "    \"\"\"\n",
    "    intermediate_vector = F.relu(self.fc1(x_in))\n",
    "    prediction_vector = self.fc2(intermediate_vector)\n",
    "\n",
    "    if apply_softmax:\n",
    "      prediction_vector = F.softmax(prediction_vector, dim=1)\n",
    "\n",
    "    return prediction_vector"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "class SurnamesClassifier(nn.Module):\n",
    "\n",
    "  def __init__(\n",
    "          self,\n",
    "          vocab: Vocab,\n",
    "          out_features: int,\n",
    "          embedding_dim: int = 56,\n",
    "          use_embedding: bool = True,\n",
    "          debug: bool = False,\n",
    "  ):\n",
    "    super(SurnamesClassifier, self).__init__()\n",
    "    self.use_embedding = use_embedding\n",
    "    self.debug = debug\n",
    "\n",
    "    self.embedding_dim = embedding_dim\n",
    "\n",
    "    last_conv_out_channels = 64\n",
    "    adaptive_avg_pool = 8\n",
    "\n",
    "    self.embedding = nn.Embedding(num_embeddings=vocab.max_seq_len+1, embedding_dim=embedding_dim)\n",
    "    self.features = nn.Sequential(\n",
    "      nn.Conv1d(in_channels=embedding_dim, out_channels=64, kernel_size=3),\n",
    "      nn.BatchNorm1d(num_features=64),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool1d(kernel_size=2),\n",
    "      nn.Conv1d(in_channels=64, out_channels=last_conv_out_channels, kernel_size=3),\n",
    "      nn.BatchNorm1d(num_features=last_conv_out_channels),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool1d(kernel_size=2),\n",
    "    )\n",
    "    # Единственный полезный (и понятный зачем) слой. Зачем? - Позволяет не думать о размерностях\n",
    "    self.avgpool = nn.AdaptiveAvgPool1d(adaptive_avg_pool)\n",
    "    self.classifier = nn.Sequential(\n",
    "      nn.Linear(last_conv_out_channels * adaptive_avg_pool, 256),\n",
    "      nn.ReLU(),\n",
    "      nn.Dropout(),\n",
    "      nn.Linear(256, out_features),\n",
    "    )\n",
    "\n",
    "    if self.debug:\n",
    "      self.forward = self._debug_forward\n",
    "    else:\n",
    "      self.forward = self._forward\n",
    "\n",
    "  def _forward(self, x: torch.Tensor):\n",
    "    # print(x.shape)\n",
    "    if self.use_embedding:\n",
    "      x = self.embedding(x)\n",
    "    # else:\n",
    "    #   # Для эксперимента с one-hot - что будет, если растянуть вектора до размера embedding_dim?\n",
    "    #   # Ответ: ничего\n",
    "    #   x = F.pad(x, (0, self.embedding_dim - x.size(2), 0, 0), value=0)\n",
    "\n",
    "    # (batch_size, num_features [embedding_dim], n_tokens)\n",
    "    x = x.reshape(x.size(0), x.size(2), x.size(1))\n",
    "    x = self.features(x)\n",
    "    x = self.avgpool(x)\n",
    "    x = torch.flatten(x, 1)\n",
    "    x = self.classifier(x)\n",
    "\n",
    "    return torch.log_softmax(x, dim=1)\n",
    "\n",
    "  def _debug_forward(self, x: torch.Tensor):\n",
    "    print(\"x: \", x.size())\n",
    "    if self.use_embedding:\n",
    "      x = self.embedding(x)\n",
    "      print(\"embedding: \", x.size())\n",
    "    else:\n",
    "      x = F.pad(x, (0, self.embedding_dim - x.size(2), 0, 0), value=0)\n",
    "      print(\"pad: \", x.size())\n",
    "\n",
    "    x = x.reshape(x.size(0), x.size(2), x.size(1))\n",
    "    print(\"reshape: \", x.size())\n",
    "    x = self.features(x)\n",
    "    print(\"features: \", x.size())\n",
    "    x = self.avgpool(x)\n",
    "    print(\"avgpool: \", x.size())\n",
    "    x = torch.flatten(x, 1)\n",
    "    print(\"flatten: \", x.size())\n",
    "    x = self.classifier(x)\n",
    "    print(\"classifier: \", x.size())\n",
    "    return torch.log_softmax(x, dim=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "common_net = SurnamesClassifier(vocab, len(surname_dict))\n",
    "\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(common_net.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, Subset\n",
    "import typing as t\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "def on_cuda(device: str) -> bool:\n",
    "  return device == \"cuda\"\n",
    "\n",
    "\n",
    "def common_train(\n",
    "        model: nn.Module,\n",
    "        loss_fn: nn.Module,\n",
    "        optimizer: optim.Optimizer,\n",
    "        train_dataloader: DataLoader,\n",
    "        epochs: int,\n",
    "        test_dataloader: DataLoader = None,\n",
    "        lr_scheduler=None,\n",
    "        verbose: int = 100,\n",
    "        device: str = \"cpu\",\n",
    ") -> t.List[float]:\n",
    "  train_losses = []\n",
    "  for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}\\n\" + \"-\" * 32)\n",
    "    train_loss = train_loop(\n",
    "      train_dataloader,\n",
    "      model,\n",
    "      loss_fn,\n",
    "      optimizer,\n",
    "      verbose=verbose,\n",
    "      device=device,\n",
    "    )\n",
    "    train_losses.append(train_loss.item())\n",
    "    if test_dataloader:\n",
    "      loss, acc = test_loop(test_dataloader, model, loss_fn, device=device)\n",
    "      if lr_scheduler:\n",
    "        lr_scheduler.step(loss)\n",
    "    torch.cuda.empty_cache()\n",
    "  return train_losses\n",
    "\n",
    "\n",
    "def train_loop(\n",
    "        dataloader: DataLoader,\n",
    "        model: nn.Module,\n",
    "        loss_fn: nn.Module,\n",
    "        optimizer: optim.Optimizer,\n",
    "        verbose: int = 100,\n",
    "        device: str = \"cpu\",\n",
    ") -> torch.Tensor:\n",
    "  model.train()\n",
    "\n",
    "  size = len(dataloader.dataset)  # noqa\n",
    "  num_batches = len(dataloader)\n",
    "  avg_loss = 0\n",
    "\n",
    "\n",
    "  for batch, (x, y) in enumerate(dataloader):\n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "    pred = model(x)\n",
    "    loss = loss_fn(pred, y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    avg_loss += loss\n",
    "    if batch % verbose == 0:\n",
    "      print(f\"loss: {loss:>7f}  [{batch * len(x):>5d}/{size:>5d}]\")\n",
    "\n",
    "    del x, y, pred, loss\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "  return avg_loss / num_batches\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_loop(\n",
    "        dataloader: DataLoader,\n",
    "        model: nn.Module,\n",
    "        loss_fn: nn.Module,\n",
    "        device: str = \"cpu\",\n",
    ") -> t.Tuple[torch.Tensor, torch.Tensor]:\n",
    "  model.eval()\n",
    "\n",
    "  size = len(dataloader.dataset)  # noqa\n",
    "  num_batches = len(dataloader)\n",
    "  avg_loss, correct = 0, 0\n",
    "\n",
    "  for x, y in dataloader:\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    pred = model(x)\n",
    "    avg_loss += loss_fn(pred, y)\n",
    "    correct += (pred.argmax(1) == y).type(torch.float).sum().item()  # noqa\n",
    "\n",
    "    del x, y, pred\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "  avg_loss /= num_batches\n",
    "  accuracy = correct / size\n",
    "  print(f\"Test Error: \\n Accuracy: {accuracy:>4f}, Avg loss: {avg_loss:>8f} \\n\")\n",
    "\n",
    "  return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def train_test_split(dataset: t.Union[Dataset, t.Sized], train_part: float) -> t.Tuple[Subset, Subset]:\n",
    "  train_size = round(train_part * len(dataset))\n",
    "  test_size = len(dataset) - train_size\n",
    "  train_dataset, test_dataset = random_split(dataset, lengths=(train_size, test_size))\n",
    "  return train_dataset, test_dataset\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_y_test_y_pred(\n",
    "        model: nn.Module,\n",
    "        test_dataloader: DataLoader,\n",
    "        device: str = \"cpu\",\n",
    ") -> t.Tuple[torch.Tensor, torch.Tensor]:\n",
    "  model.eval()\n",
    "\n",
    "  y_test = []\n",
    "  y_pred = []\n",
    "  for x, y in test_dataloader:\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    pred = model(x).argmax(1)\n",
    "    y_test.append(y)\n",
    "    y_pred.append(pred)\n",
    "\n",
    "    del x\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "  return torch.hstack(y_test).detach().cpu(), torch.hstack(y_pred).detach().cpu()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------------------------\n",
      "loss: 3.112618  [    0/ 8784]\n",
      "loss: 3.026118  [ 4000/ 8784]\n",
      "loss: 1.229229  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.548270, Avg loss: 1.595330 \n",
      "\n",
      "Epoch 2\n",
      "--------------------------------\n",
      "loss: 1.313912  [    0/ 8784]\n",
      "loss: 0.992270  [ 4000/ 8784]\n",
      "loss: 1.251219  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.574681, Avg loss: 1.456847 \n",
      "\n",
      "Epoch 3\n",
      "--------------------------------\n",
      "loss: 1.970159  [    0/ 8784]\n",
      "loss: 1.351185  [ 4000/ 8784]\n",
      "loss: 1.808311  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.596084, Avg loss: 1.393186 \n",
      "\n",
      "Epoch 4\n",
      "--------------------------------\n",
      "loss: 1.332764  [    0/ 8784]\n",
      "loss: 0.734504  [ 4000/ 8784]\n",
      "loss: 1.124861  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.605191, Avg loss: 1.346658 \n",
      "\n",
      "Epoch 5\n",
      "--------------------------------\n",
      "loss: 0.415921  [    0/ 8784]\n",
      "loss: 1.039611  [ 4000/ 8784]\n",
      "loss: 0.977130  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.612477, Avg loss: 1.352463 \n",
      "\n",
      "Epoch 6\n",
      "--------------------------------\n",
      "loss: 1.267020  [    0/ 8784]\n",
      "loss: 0.862829  [ 4000/ 8784]\n",
      "loss: 0.929966  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.622951, Avg loss: 1.302202 \n",
      "\n",
      "Epoch 7\n",
      "--------------------------------\n",
      "loss: 1.176860  [    0/ 8784]\n",
      "loss: 0.418745  [ 4000/ 8784]\n",
      "loss: 0.291171  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.638434, Avg loss: 1.310674 \n",
      "\n",
      "Epoch 8\n",
      "--------------------------------\n",
      "loss: 0.565991  [    0/ 8784]\n",
      "loss: 0.465393  [ 4000/ 8784]\n",
      "loss: 0.712209  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.631603, Avg loss: 1.283153 \n",
      "\n",
      "Epoch 9\n",
      "--------------------------------\n",
      "loss: 2.687658  [    0/ 8784]\n",
      "loss: 0.869978  [ 4000/ 8784]\n",
      "loss: 1.464135  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.648907, Avg loss: 1.266997 \n",
      "\n",
      "Epoch 10\n",
      "--------------------------------\n",
      "loss: 0.979360  [    0/ 8784]\n",
      "loss: 0.116383  [ 4000/ 8784]\n",
      "loss: 0.590378  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.646630, Avg loss: 1.323034 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "common_net.use_embedding = False\n",
    "_ = common_train(\n",
    "  epochs=10,\n",
    "  model=common_net,\n",
    "  loss_fn=loss_fn,\n",
    "  optimizer=optimizer,\n",
    "  train_dataloader=DataLoader(index_dataset_train, batch_size=8, shuffle=True),\n",
    "  test_dataloader=DataLoader(index_dataset_test, batch_size=512),\n",
    "  verbose=500,\n",
    "  device=DEVICE,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------------------------\n",
      "(0, [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([5, 4, 2, 9, 6, 2, 3, 6])])\n",
      "Batch index:  0\n",
      "Batch size:  torch.Size([8, 17, 56])\n",
      "Batch label:  tensor([ 4,  3,  5,  2, 15,  4, 12,  0])\n",
      "Epoch 2\n",
      "--------------------------------\n",
      "(0, [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 1., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([2, 9, 0, 0, 2, 0, 0, 2])])\n",
      "Batch index:  0\n",
      "Batch size:  torch.Size([8, 17, 56])\n",
      "Batch label:  tensor([0, 7, 0, 3, 3, 2, 2, 3])\n",
      "Epoch 3\n",
      "--------------------------------\n",
      "(0, [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([3, 8, 2, 3, 1, 7, 8, 3])])\n",
      "Batch index:  0\n",
      "Batch size:  torch.Size([8, 17, 56])\n",
      "Batch label:  tensor([ 0, 13, 11,  0,  4,  3,  0,  0])\n",
      "Epoch 4\n",
      "--------------------------------\n",
      "(0, [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([ 3,  2,  2,  0,  9, 16,  2,  0])])\n",
      "Batch index:  0\n",
      "Batch size:  torch.Size([8, 17, 56])\n",
      "Batch label:  tensor([3, 4, 0, 0, 7, 0, 9, 0])\n",
      "Epoch 5\n",
      "--------------------------------\n",
      "(0, [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([13,  3,  0,  2,  3,  3,  3,  0])])\n",
      "Batch index:  0\n",
      "Batch size:  torch.Size([8, 17, 56])\n",
      "Batch label:  tensor([9, 8, 6, 0, 0, 4, 8, 0])\n",
      "Epoch 6\n",
      "--------------------------------\n",
      "(0, [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([ 2,  6, 17,  0,  2,  2,  3,  0])])\n",
      "Batch index:  0\n",
      "Batch size:  torch.Size([8, 17, 56])\n",
      "Batch label:  tensor([3, 0, 2, 3, 1, 6, 0, 0])\n",
      "Epoch 7\n",
      "--------------------------------\n",
      "(0, [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([ 2,  9, 13,  3,  0,  0,  0,  4])])\n",
      "Batch index:  0\n",
      "Batch size:  torch.Size([8, 17, 56])\n",
      "Batch label:  tensor([ 0,  7,  9,  0, 17,  3,  2,  2])\n",
      "Epoch 8\n",
      "--------------------------------\n",
      "(0, [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([13,  0,  3,  0,  0,  3,  3,  2])])\n",
      "Batch index:  0\n",
      "Batch size:  torch.Size([8, 17, 56])\n",
      "Batch label:  tensor([ 1, 17,  9,  6,  0,  0, 12,  2])\n",
      "Epoch 9\n",
      "--------------------------------\n",
      "(0, [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([ 4,  9,  0, 11,  2,  8,  3,  0])])\n",
      "Batch index:  0\n",
      "Batch size:  torch.Size([8, 17, 56])\n",
      "Batch label:  tensor([ 2,  0,  0,  0,  0,  6,  3, 11])\n",
      "Epoch 10\n",
      "--------------------------------\n",
      "(0, [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([ 3,  2,  0,  3,  3,  2,  0, 13])])\n",
      "Batch index:  0\n",
      "Batch size:  torch.Size([8, 17, 56])\n",
      "Batch label:  tensor([0, 3, 9, 0, 9, 0, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "dataloader = DataLoader(index_dataset_train, batch_size=8, shuffle=True)\n",
    "for epoch in range(10):\n",
    "  print(f\"Epoch {epoch + 1}\\n\" + \"-\" * 32)\n",
    "  # train_loss = train_loop(\n",
    "  #   train_dataloader,\n",
    "  #   model,\n",
    "  #   loss_fn,\n",
    "  #   optimizer,\n",
    "  #   verbose=verbose,\n",
    "  #   device=device,\n",
    "  # )\n",
    "  common_net.train()\n",
    "\n",
    "  size = len(dataloader.dataset)  # noqa\n",
    "  num_batches = len(dataloader)\n",
    "  avg_loss = 0\n",
    "  print(next(enumerate(dataloader)))\n",
    "\n",
    "  for idx, batch in enumerate(dataloader):\n",
    "    print('Batch index: ', idx)\n",
    "    print('Batch size: ', batch[0].size())\n",
    "    print('Batch label: ', batch[1])\n",
    "    break\n",
    "\n",
    "\n",
    "  # for batch, (x, y) in enumerate(dataloader):\n",
    "  #   print(batch)\n",
    "  #   # x, y = x.to(device), y.to(device)\n",
    "  #\n",
    "  #   pred = common_net(x)\n",
    "  #   loss = loss_fn(pred, y)\n",
    "  #\n",
    "  #   optimizer.zero_grad()\n",
    "  #   loss.backward()\n",
    "  #   optimizer.step()\n",
    "  #\n",
    "  #   avg_loss += loss\n",
    "  #   if batch % 500 == 0:\n",
    "  #     print(f\"loss: {loss:>7f}  [{batch * len(x):>5d}/{size:>5d}]\")\n",
    "  #\n",
    "  #   del x, y, pred, loss\n",
    "  #   torch.cuda.empty_cache()\n",
    "  #\n",
    "  # train_losses.append(avg_loss / num_batches.item())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "embeddings_net = SurnamesClassifier(vocab, len(surname_dict))\n",
    "\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(embeddings_net.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------------------------\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "File \u001B[1;32m<timed exec>:3\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n",
      "Input \u001B[1;32mIn [208]\u001B[0m, in \u001B[0;36mcommon_train\u001B[1;34m(model, loss_fn, optimizer, train_dataloader, epochs, test_dataloader, lr_scheduler, verbose, device)\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[0;32m     23\u001B[0m   \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m32\u001B[39m)\n\u001B[1;32m---> 24\u001B[0m   train_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     25\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     26\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     27\u001B[0m \u001B[43m    \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     28\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     29\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     30\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     31\u001B[0m \u001B[43m  \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     32\u001B[0m   train_losses\u001B[38;5;241m.\u001B[39mappend(train_loss\u001B[38;5;241m.\u001B[39mitem())\n\u001B[0;32m     33\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m test_dataloader:\n",
      "Input \u001B[1;32mIn [208]\u001B[0m, in \u001B[0;36mtrain_loop\u001B[1;34m(dataloader, model, loss_fn, optimizer, verbose, device)\u001B[0m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch, (x, y) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(dataloader):\n\u001B[0;32m     57\u001B[0m   x, y \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mto(device), y\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m---> 59\u001B[0m   pred \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     60\u001B[0m   loss \u001B[38;5;241m=\u001B[39m loss_fn(pred, y)\n\u001B[0;32m     62\u001B[0m   optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[1;32mF:\\Programming\\Python\\DAaML\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Input \u001B[1;32mIn [233]\u001B[0m, in \u001B[0;36mSurnamesClassifier._forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     45\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_forward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: torch\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[0;32m     46\u001B[0m   \u001B[38;5;66;03m# print(x.shape)\u001B[39;00m\n\u001B[0;32m     47\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muse_embedding:\n\u001B[1;32m---> 48\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     49\u001B[0m   \u001B[38;5;66;03m# else:\u001B[39;00m\n\u001B[0;32m     50\u001B[0m   \u001B[38;5;66;03m#   # Для эксперимента с one-hot - что будет, если растянуть вектора до размера embedding_dim?\u001B[39;00m\n\u001B[0;32m     51\u001B[0m   \u001B[38;5;66;03m#   # Ответ: ничего\u001B[39;00m\n\u001B[0;32m     52\u001B[0m   \u001B[38;5;66;03m#   x = F.pad(x, (0, self.embedding_dim - x.size(2), 0, 0), value=0)\u001B[39;00m\n\u001B[0;32m     53\u001B[0m \n\u001B[0;32m     54\u001B[0m   \u001B[38;5;66;03m# (batch_size, num_features [embedding_dim], n_tokens)\u001B[39;00m\n\u001B[0;32m     55\u001B[0m   x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mreshape(x\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m), x\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m2\u001B[39m), x\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m1\u001B[39m))\n",
      "File \u001B[1;32mF:\\Programming\\Python\\DAaML\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mF:\\Programming\\Python\\DAaML\\venv\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:160\u001B[0m, in \u001B[0;36mEmbedding.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    159\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 160\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    161\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_norm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    162\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnorm_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mF:\\Programming\\Python\\DAaML\\venv\\lib\\site-packages\\torch\\nn\\functional.py:2210\u001B[0m, in \u001B[0;36membedding\u001B[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001B[0m\n\u001B[0;32m   2204\u001B[0m     \u001B[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001B[39;00m\n\u001B[0;32m   2205\u001B[0m     \u001B[38;5;66;03m# XXX: equivalent to\u001B[39;00m\n\u001B[0;32m   2206\u001B[0m     \u001B[38;5;66;03m# with torch.no_grad():\u001B[39;00m\n\u001B[0;32m   2207\u001B[0m     \u001B[38;5;66;03m#   torch.embedding_renorm_\u001B[39;00m\n\u001B[0;32m   2208\u001B[0m     \u001B[38;5;66;03m# remove once script supports set_grad_enabled\u001B[39;00m\n\u001B[0;32m   2209\u001B[0m     _no_grad_embedding_renorm_(weight, \u001B[38;5;28minput\u001B[39m, max_norm, norm_type)\n\u001B[1;32m-> 2210\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mIndexError\u001B[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Embedding представление\n",
    "embeddings_net.use_embedding = True\n",
    "_ = common_train(\n",
    "  epochs=15,\n",
    "  model=embeddings_net,\n",
    "  loss_fn=loss_fn,\n",
    "  optimizer=optimizer,\n",
    "  train_dataloader=DataLoader(hot_dataset_train, batch_size=8, shuffle=True),\n",
    "  test_dataloader=DataLoader(hot_dataset_test, batch_size=512),\n",
    "  verbose=500,\n",
    "  device=DEVICE,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uo-hf5CQ0iWv"
   },
   "source": [
    "## 3. Классификация обзоров на фильмы (ConvNet)\n",
    "\n",
    "Датасет: https://disk.yandex.ru/d/tdinpb0nN_Dsrg\n",
    "\n",
    "2.1 Создайте набор данных на основе файлов polarity/positive_reviews.csv (положительные отзывы) и polarity/negative_reviews.csv (отрицательные отзывы). Разбейте на обучающую и тестовую выборку.\n",
    "  * токен = __слово__\n",
    "  * данные для обучения в датасете представляются в виде последовательности индексов токенов\n",
    "  * словарь создается на основе _только_ обучающей выборки. Для корректной обработки ситуаций, когда в тестовой выборке встретится токен, который не хранится в словаре, добавьте в словарь специальный токен `<UNK>`\n",
    "  * добавьте предобработку текста\n",
    "\n",
    "2.2. Обучите классификатор.\n",
    "  \n",
    "  * Для преобразования последовательности индексов в последовательность векторов используйте `nn.Embedding` \n",
    "    - подберите адекватную размерность вектора эмбеддинга: \n",
    "    - модуль `nn.Embedding` обучается\n",
    "\n",
    "  * Используйте одномерные свертки и пулинг (`nn.Conv1d`, `nn.MaxPool1d`)\n",
    "    - обратите внимание, что `nn.Conv1d` ожидает на вход трехмерный тензор размерности `(batch, embedding_dim, seq_len)`\n",
    "\n",
    "\n",
    "2.7 Измерить точность на тестовой выборке. Проверить работоспособность модели: придумать небольшой отзыв, прогнать его через модель и вывести номер предсказанного класса (сделать это для явно позитивного и явно негативного отзыва)\n",
    "* Целевое значение accuracy на валидации - 70+%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "positive_raw = pd.read_csv(\"./polarity/positive_reviews.csv\", header=None)\n",
    "negative_raw = pd.read_csv(\"./polarity/negative_reviews.csv\", header=None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "positive_raw.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "negative_raw.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "positive_raw[\"state\"] = 1\n",
    "negative_raw[\"state\"] = 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "negative_raw"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "positive_raw"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_reviews = pd.concat([negative_raw, positive_raw], axis=0)\n",
    "all_reviews.columns = [\"review\", \"rating\"]\n",
    "all_reviews"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(all_reviews[\"review\"], all_reviews[\"rating\"], test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nltk.word_tokenize(all_reviews.review)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_reviews[\"review\"].flatten()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
