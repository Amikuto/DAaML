{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Создание нейронной сети без использования готовых решений\n",
    "\n",
    "__Автор__: Никита Владимирович Блохин (NVBlokhin@fa.ru)\n",
    "\n",
    "Финансовый университет, 2020 г. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PqC4R7SGseKa"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0J2RM8f5wP33"
   },
   "source": [
    "## 1. Создание нейронов и полносвязных слоев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2ArJn_nsdZC"
   },
   "source": [
    "1.1. Используя операции над матрицами и векторами из библиотеки `torch`, реализовать нейрон с заданными весами `weights` и `bias`. Прогнать вектор `inputs` через нейрон и вывести результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "\n",
    "  def __init__(self, weights, bias):\n",
    "    self.bias = bias\n",
    "    self.weights = weights\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    z = self.bias + torch.sum(self.weights * inputs)\n",
    "    return z\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [],
   "source": [
    "inputs = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "weights = torch.tensor([-0.2, 0.3, -0.5, 0.7])\n",
    "bias = 3.14"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(4.8400)"
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuron = Neuron(weights, bias)\n",
    "out = neuron.forward(inputs)\n",
    "out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1.2 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать полносвязный слой с заданными весами `weights` и `biases`. Прогнать вектор `inputs` через слой и вывести результат."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "outputs": [],
   "source": [
    "class Linear:\n",
    "  biases = torch.tensor([])\n",
    "  weights = torch.tensor([])\n",
    "\n",
    "  def __init__(self, weights, bias):\n",
    "    self.biases = bias\n",
    "    self.weights = weights\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    # if len(inputs.shape) == 1:\n",
    "    #   shaped = inputs.view(inputs.shape[0], 1)\n",
    "    # else:\n",
    "    #   shaped = inputs.T\n",
    "    return (inputs @ self.weights) + biases\n",
    "\n",
    "    # return torch.sum(mp, dim=0) + biases"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "outputs": [],
   "source": [
    "class Linear2:\n",
    "  biases = torch.tensor([])\n",
    "  weights = torch.tensor([])\n",
    "\n",
    "  def __init__(self, weights, bias):\n",
    "    self.biases = bias\n",
    "    self.weights = weights\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    shaped = inputs.view(inputs.shape[0], 1)\n",
    "    mp = shaped * weights\n",
    "    su = torch.sum(mp, dim=1)\n",
    "    return su.view(inputs.shape[0], 1) + biases"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "outputs": [],
   "source": [
    "inputs = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "weights = torch.tensor([[-0.2, 0.3, -0.5, 0.7],\n",
    "                        [0.5, -0.91, 0.26, -0.5],\n",
    "                        [-0.26, -0.27, 0.17, 0.87]]).T\n",
    "\n",
    "biases = torch.tensor([3.14, 2.71, 7.2])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 4.8400,  0.1700, 10.3900])"
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = Linear(weights, biases)\n",
    "out = linear.forward(inputs)\n",
    "out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 3.1800,  2.7500,  7.2400],\n        [ 1.3800,  0.9500,  5.4400],\n        [ 2.9300,  2.5000,  6.9900],\n        [ 7.4200,  6.9900, 11.4800]])"
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear2 = Linear2(weights, biases)\n",
    "out2 = linear2.forward(inputs)\n",
    "out2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([-0.2000,  0.6000, -1.5000,  2.8000])"
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs * torch.tensor([-0.2, 0.3, -0.5, 0.7])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(1.7000)"
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(inputs * torch.tensor([-0.2, 0.3, -0.5, 0.7]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.2000,  0.5000, -0.2600],\n        [ 0.6000, -1.8200, -0.5400],\n        [-1.5000,  0.7800,  0.5100],\n        [ 2.8000, -2.0000,  3.4800]])"
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.view(inputs.shape[0], 1) * weights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 1.0000,  2.0000,  3.0000,  2.5000],\n        [ 2.0000,  5.0000, -1.0000,  2.0000],\n        [-1.5000,  2.7000,  3.3000, -0.8000]])"
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.2000,  0.5000, -0.2600],\n        [ 0.3000, -0.9100, -0.2700],\n        [-0.5000,  0.2600,  0.1700],\n        [ 0.7000, -0.5000,  0.8700]])"
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.2000,  1.0000,  0.3900],\n        [ 0.6000, -4.5500, -0.7290],\n        [-1.5000, -0.2600,  0.5610],\n        [ 1.7500, -1.0000, -0.6960]])"
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.T * weights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.6500, -1.7900,  1.8850],\n        [ 3.0000, -4.8100, -0.3000],\n        [-1.1000, -1.9490, -0.4740]])"
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(inputs, weights)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(-4.6340)"
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(inputs.T * weights)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 1.7000, -2.5400,  3.1900])"
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(inputs.view(inputs.shape[0], 1) * weights, dim=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 1.0000,  2.0000,  3.0000,  2.5000],\n        [ 2.0000,  5.0000, -1.0000,  2.0000],\n        [-1.5000,  2.7000,  3.3000, -0.8000]])"
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.2000,  0.5000, -0.2600],\n        [ 0.3000, -0.9100, -0.2700],\n        [-0.5000,  0.2600,  0.1700],\n        [ 0.7000, -0.5000,  0.8700]])"
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 1.7000, -2.5400,  3.1900])"
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(inputs, weights)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1.],\n        [2.],\n        [3.],\n        [4.]])"
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shaped = inputs.view(inputs.shape[0], 1)\n",
    "# shaped = inputs.T\n",
    "shaped"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.2000,  0.5000, -0.2600],\n        [ 0.6000, -1.8200, -0.5400],\n        [-1.5000,  0.7800,  0.5100],\n        [ 2.8000, -2.0000,  3.4800]])"
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp = shaped * weights\n",
    "mp"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 0.0400, -1.7600, -0.2100,  4.2800])"
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "su = torch.sum(mp, dim=1)\n",
    "su"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([3.1400, 2.7100, 7.2000])"
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 3.1800,  2.7500,  7.2400],\n        [ 1.3800,  0.9500,  5.4400],\n        [ 2.9300,  2.5000,  6.9900],\n        [ 7.4200,  6.9900, 11.4800]])"
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "su.view(inputs.shape[0], 1) + biases"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.1800, 2.7500, 7.2400])\n",
      "tensor([1.3800, 0.9500, 5.4400])\n",
      "tensor([2.9300, 2.5000, 6.9900])\n",
      "tensor([ 7.4200,  6.9900, 11.4800])\n"
     ]
    }
   ],
   "source": [
    "for i in su:\n",
    "  print(i + biases)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 2.9400,  3.2100,  6.9400],\n        [ 3.7400,  0.8900,  6.6600],\n        [ 1.6400,  3.4900,  7.7100],\n        [ 5.9400,  0.7100, 10.6800]])"
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bimp = biases + mp\n",
    "bimp"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1.3 Реализовать полносвязный слой из __2.1.2__ таким образом, чтобы он мог принимать на вход матрицу (батч) с данными. Продемонстрировать работу.\n",
    "Результатом прогона сквозь слой должна быть матрица размера `batch_size` x `n_neurons`.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[1, 2, 3, 2.5],\n",
    "                       [2, 5, -1, 2],\n",
    "                       [-1.5, 2.7, 3.3, -0.8]])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.2000,  0.6000, -1.5000,  1.7500],\n        [ 1.0000, -4.5500, -0.2600, -1.0000],\n        [ 0.3900, -0.7290,  0.5610, -0.6960]])"
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.T * inputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([3.1400, 2.7100, 7.2000])"
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 1.0000,  2.0000, -1.5000],\n        [ 2.0000,  5.0000,  2.7000],\n        [ 3.0000, -1.0000,  3.3000],\n        [ 2.5000,  2.0000, -0.8000]])"
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shaped = inputs.T\n",
    "shaped"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.2000,  0.5000, -0.2600],\n        [ 0.3000, -0.9100, -0.2700],\n        [-0.5000,  0.2600,  0.1700],\n        [ 0.7000, -0.5000,  0.8700]])"
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor([[ 1.0000],\n",
      "        [ 2.0000],\n",
      "        [-1.5000]])\n",
      "tensor([-0.2000,  0.5000, -0.2600])\n",
      "tensor([[-0.2000,  0.5000, -0.2600],\n",
      "        [-0.4000,  1.0000, -0.5200],\n",
      "        [ 0.3000, -0.7500,  0.3900]])\n",
      "tensor([ 0.0400,  0.0800, -0.0600])\n",
      "1\n",
      "tensor([[2.0000],\n",
      "        [5.0000],\n",
      "        [2.7000]])\n",
      "tensor([ 0.3000, -0.9100, -0.2700])\n",
      "tensor([[ 0.6000, -1.8200, -0.5400],\n",
      "        [ 1.5000, -4.5500, -1.3500],\n",
      "        [ 0.8100, -2.4570, -0.7290]])\n",
      "tensor([-1.7600, -4.4000, -2.3760])\n",
      "2\n",
      "tensor([[ 3.0000],\n",
      "        [-1.0000],\n",
      "        [ 3.3000]])\n",
      "tensor([-0.5000,  0.2600,  0.1700])\n",
      "tensor([[-1.5000,  0.7800,  0.5100],\n",
      "        [ 0.5000, -0.2600, -0.1700],\n",
      "        [-1.6500,  0.8580,  0.5610]])\n",
      "tensor([-0.2100,  0.0700, -0.2310])\n",
      "3\n",
      "tensor([[ 2.5000],\n",
      "        [ 2.0000],\n",
      "        [-0.8000]])\n",
      "tensor([ 0.7000, -0.5000,  0.8700])\n",
      "tensor([[ 1.7500, -1.2500,  2.1750],\n",
      "        [ 1.4000, -1.0000,  1.7400],\n",
      "        [-0.5600,  0.4000, -0.6960]])\n",
      "tensor([ 2.6750,  2.1400, -0.8560])\n"
     ]
    }
   ],
   "source": [
    "for i in range(shaped.size()[0]):\n",
    "  print(i)\n",
    "  print(shaped[i].view(inputs.shape[0], 1)) #shaped[i])\n",
    "  print(weights[i])#weights[i].view(inputs.shape[0], 1))\n",
    "  print(weights[i] * shaped[i].view(inputs.shape[0], 1))\n",
    "  print(torch.sum(weights[i] * shaped[i].view(inputs.shape[0], 1), dim=1))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "tensor([[-0.2000,  0.3000, -0.5000,  0.7000],\n",
      "        [ 0.5000, -0.9100,  0.2600, -0.5000],\n",
      "        [-0.2600, -0.2700,  0.1700,  0.8700]])\n",
      "tensor(2.)\n",
      "tensor([[-0.4000,  0.6000, -1.0000,  1.4000],\n",
      "        [ 1.0000, -1.8200,  0.5200, -1.0000],\n",
      "        [-0.5200, -0.5400,  0.3400,  1.7400]])\n",
      "tensor(-1.5000)\n",
      "tensor([[ 0.3000, -0.4500,  0.7500, -1.0500],\n",
      "        [-0.7500,  1.3650, -0.3900,  0.7500],\n",
      "        [ 0.3900,  0.4050, -0.2550, -1.3050]])\n",
      "\n",
      "\n",
      "tensor(2.)\n",
      "tensor([[-0.4000,  0.6000, -1.0000,  1.4000],\n",
      "        [ 1.0000, -1.8200,  0.5200, -1.0000],\n",
      "        [-0.5200, -0.5400,  0.3400,  1.7400]])\n",
      "tensor(5.)\n",
      "tensor([[-1.0000,  1.5000, -2.5000,  3.5000],\n",
      "        [ 2.5000, -4.5500,  1.3000, -2.5000],\n",
      "        [-1.3000, -1.3500,  0.8500,  4.3500]])\n",
      "tensor(2.7000)\n",
      "tensor([[-0.5400,  0.8100, -1.3500,  1.8900],\n",
      "        [ 1.3500, -2.4570,  0.7020, -1.3500],\n",
      "        [-0.7020, -0.7290,  0.4590,  2.3490]])\n",
      "\n",
      "\n",
      "tensor(3.)\n",
      "tensor([[-0.6000,  0.9000, -1.5000,  2.1000],\n",
      "        [ 1.5000, -2.7300,  0.7800, -1.5000],\n",
      "        [-0.7800, -0.8100,  0.5100,  2.6100]])\n",
      "tensor(-1.)\n",
      "tensor([[ 0.2000, -0.3000,  0.5000, -0.7000],\n",
      "        [-0.5000,  0.9100, -0.2600,  0.5000],\n",
      "        [ 0.2600,  0.2700, -0.1700, -0.8700]])\n",
      "tensor(3.3000)\n",
      "tensor([[-0.6600,  0.9900, -1.6500,  2.3100],\n",
      "        [ 1.6500, -3.0030,  0.8580, -1.6500],\n",
      "        [-0.8580, -0.8910,  0.5610,  2.8710]])\n",
      "\n",
      "\n",
      "tensor(2.5000)\n",
      "tensor([[-0.5000,  0.7500, -1.2500,  1.7500],\n",
      "        [ 1.2500, -2.2750,  0.6500, -1.2500],\n",
      "        [-0.6500, -0.6750,  0.4250,  2.1750]])\n",
      "tensor(2.)\n",
      "tensor([[-0.4000,  0.6000, -1.0000,  1.4000],\n",
      "        [ 1.0000, -1.8200,  0.5200, -1.0000],\n",
      "        [-0.5200, -0.5400,  0.3400,  1.7400]])\n",
      "tensor(-0.8000)\n",
      "tensor([[ 0.1600, -0.2400,  0.4000, -0.5600],\n",
      "        [-0.4000,  0.7280, -0.2080,  0.4000],\n",
      "        [ 0.2080,  0.2160, -0.1360, -0.6960]])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in shaped:\n",
    "  for j in i:\n",
    "    print(j)\n",
    "    # print(i)\n",
    "    print(j * weights.T)\n",
    "  # mp = weights.T * i\n",
    "  # print(mp)\n",
    "  # su = torch.sum(mp, dim=0)\n",
    "  #\n",
    "  # bisu = biases + su\n",
    "  # print(bisu)\n",
    "  print()\n",
    "  print()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.2000,  1.0000,  0.3900],\n        [ 0.6000, -4.5500, -0.7290],\n        [-1.5000, -0.2600,  0.5610],\n        [ 1.7500, -1.0000, -0.6960]])"
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp = shaped * weights\n",
    "mp"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [],
   "source": [
    "out2 = linear.forward(inputs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([14.2400,  8.3710, 11.8510, 13.1040])"
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1.4 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать полносвязный слой из `n_neurons` нейронов с `n_features` весами у каждого нейрона (инициализируются из стандартного нормального распределения). Прогнать вектор `inputs` через слой и вывести результат. Результатом прогона сквозь слой должна быть матрица размера `batch_size` x `n_neurons`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Linear:\n",
    "  def __init__(self, n_features, n_neurons):\n",
    "    # <создать атрибуты объекта weights и biases>\n",
    "    pass\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    return # <реализовать логику слоя>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1.5 Используя решение из __1.4__, создать 2 полносвязных слоя и пропустить матрицу `inputs` последовательно через эти два слоя. Количество нейронов в первом слое выбрать произвольно, количество нейронов во втором слое выбрать так, чтобы результатом прогона являлась матрица (3x7)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[1, 2, 3, 2.5],\n",
    "                       [2, 5, -1, 2],\n",
    "                       [-1.5, 2.7, 3.3, -0.8]])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRVH_2K7xTBC"
   },
   "source": [
    "## 2. Создание функций активации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9kngE6Fxs9D"
   },
   "source": [
    "2.1 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию активации ReLU:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/f4353f4e3e484130504049599d2e7b040793e1eb)\n",
    "\n",
    "Создать матрицу размера (4,3), заполненную числами из стандартного нормального распределения, и проверить работоспособность функции активации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "jZLvMRByxSTC"
   },
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "  def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.where(inputs>0, inputs, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.5473, -0.8577, -0.8484],\n        [-0.2296,  0.4201, -1.2862],\n        [ 0.4745,  0.0710, -1.8583],\n        [-0.5698,  1.1932,  1.0674]])"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randn((4, 3))\n",
    "input"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.0000, 0.0000, 0.0000],\n        [0.0000, 0.4201, 0.0000],\n        [0.4745, 0.0710, 0.0000],\n        [0.0000, 1.1932, 1.0674]])"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = ReLU()\n",
    "r.forward(input)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "puExCWiKyTtb"
   },
   "source": [
    "2.2 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию активации softmax:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/6d7500d980c313da83e4117da701bf7c8f1982f5)\n",
    "\n",
    "Создать матрицу размера (4,3), заполненную числами из стандартного нормального распределения, и проверить работоспособность функции активации. Строки матрицы трактовать как выходы линейного слоя некоторого классификатора для 4 различных примеров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "fXNcFlqqyKHl"
   },
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "  def forward(self, inputs: torch.Tensor):\n",
    "    # out = []\n",
    "    # for row in inputs:\n",
    "    #   out.append(torch.exp(row) / torch.sum(torch.exp(row)))\n",
    "    # return out\n",
    "    return torch.exp(inputs) / torch.sum(torch.exp(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.0671,  1.3506, -0.2321],\n        [-0.7384, -0.8856, -1.3500],\n        [-1.9706, -0.2635, -0.6024],\n        [-0.6085, -1.0512,  1.7531]])"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randn((4, 3))\n",
    "input"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.0713, 0.2574, 0.0529],\n        [0.0319, 0.0275, 0.0173],\n        [0.0093, 0.0512, 0.0365],\n        [0.0363, 0.0233, 0.3850]])"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm = Softmax()\n",
    "sm.forward(input)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.4566, 0.3383, 0.2052])"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_test = torch.tensor([1.2, 0.9, 0.4])\n",
    "sm.forward(input_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxVK2TYez_Ye"
   },
   "source": [
    "2.3 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию активации ELU:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/eb23becd37c3602c4838e53f532163279192e4fd)\n",
    "\n",
    "Создать матрицу размера (4,3), заполненную числами из стандартного нормального распределения, и проверить работоспособность функции активации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "NzMz7HDLySxK"
   },
   "outputs": [],
   "source": [
    "class ELU:\n",
    "  def __init__(self, alpha):\n",
    "    self.alpha = alpha\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    return torch.where(inputs<=0, self.alpha * (torch.exp(inputs) - 1), inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.8522, -0.6605, -0.1436],\n        [ 0.1201, -0.7006,  1.4208],\n        [-0.0792, -0.5523, -0.7539],\n        [-0.8454, -0.0358,  1.6507]])"
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randn((4, 3))\n",
    "input"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.5735, -0.4834, -0.1338],\n        [ 0.1201, -0.5037,  1.4208],\n        [-0.0762, -0.4244, -0.5295],\n        [-0.5706, -0.0351,  1.6507]])"
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elu = ELU(1)\n",
    "elu.forward(input)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0peh8r-20Pof"
   },
   "source": [
    "## 3. Создание функции потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EY-k3eEs0f7f"
   },
   "source": [
    "3.1 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию потерь MSE:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/e258221518869aa1c6561bb75b99476c4734108e)\n",
    "\n",
    "Создать полносвязный слой с 1 нейроном, прогнать через него батч `inputs` и посчитать значение MSE, трактуя вектор `y` как вектор правильных ответов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f9-wdj5Tz-br"
   },
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "  def forward(self, y_pred, y_true):\n",
    "    return # <реализовать логику MSE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NAyuDU9F1Vuz"
   },
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[1, 2, 3, 2.5], \n",
    "                       [2, 5, -1, 2], \n",
    "                       [-1.5, 2.7, 3.3, -0.8]])\n",
    "\n",
    "y = torch.tensor([2, 3, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaR7rILd1eWR"
   },
   "source": [
    "3.2 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию потерь Categorical Cross-Entropy:\n",
    "\n",
    "<img src=\"https://i.ibb.co/93gy1dN/Screenshot-9.png\" width=\"200\">\n",
    "\n",
    "Создать полносвязный слой с 3 нейронами и прогнать через него батч `inputs`. Полученный результат пропустить через функцию активации softmax. Посчитать значение CCE, трактуя вектор `y` как вектор правильных ответов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hQl8pJsT3HcF"
   },
   "outputs": [],
   "source": [
    "class CategoricalCrossentropyLoss:\n",
    "  def forward(self, y_pred, y_true):\n",
    "    # <реализовать логику CCE>\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s7Qoupfo1ZGJ"
   },
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[1, 2, 3, 2.5], \n",
    "                        [2, 5, -1, 2], \n",
    "                        [-1.5, 2.7, 3.3, -0.8]])\n",
    "y = torch.tensor([1, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fA6dbanf44_4"
   },
   "source": [
    "3.3 Модифицировать 2.3.1, добавив L2-регуляризацию.\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/d92ca2429275bfdc0474523babbafe014ca8b580)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ADsZxD-h4_Os"
   },
   "outputs": [],
   "source": [
    "class MSELossL2:\n",
    "  def __init__(self, lambda_):\n",
    "    # <создать атрибут объекта alpha>\n",
    "    pass\n",
    "\n",
    "  def data_loss(self, y_pred, y_true):\n",
    "    # <подсчет первого слагаемого из формулы>\n",
    "    pass\n",
    "\n",
    "  def reg_loss(self, layer):\n",
    "    # используйте атрибуты объекта layer, в которых хранятся веса слоя\n",
    "    # <подсчет второго слагаемого из формулы>\n",
    "    pass\n",
    "\n",
    "  def forward(self, y_pred, y_true):\n",
    "    return self.data_loss(y_pred, y_true) + self.reg_loss(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w049ZSdR6qQi"
   },
   "source": [
    "## 4. Обратное распространение ошибки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBtCfSME9W7Q"
   },
   "source": [
    "4.1 Используя один нейрон и SGD (1 пример за шаг), решите задачу регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4xmI-QJ66WAF"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y, coef = make_regression(n_features=4, n_informative=4, coef=True, bias=0.5)\n",
    "X = # <преобразуйте массивы numpy в тензоры torch с типом torch.float32\n",
    "y = # <преобразуйте массивы numpy в тензоры torch с типом torch.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KpPSPYSpD9Ey"
   },
   "source": [
    "[Граф вычислений для этой задачи](https://i.ibb.co/2dhDxZx/photo-2021-02-15-17-18-04.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fc1sXtGd_J-y"
   },
   "source": [
    "4.1.1 Модифицируйте класс `MSELoss` из __2.3.1__, реализовав расчет производной относительно предыдущего слоя\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "llFigkqd_JRU"
   },
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "  def forward(self, y_pred, y_true):\n",
    "    return # <реализовать логику MSE>\n",
    "\n",
    "  def backward(self, y_pred, y_true):\n",
    "    self.dinput = # df/dc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GY7ForfM97UQ"
   },
   "source": [
    "4.1.2. Модифицируйте класс `Neuron` из __2.1.1__:\n",
    "\n",
    "  1) Сделайте так, чтобы веса нейрона инициализировались из стандартного нормального распределения\n",
    "\n",
    "  2) Реализуйте расчет градиента относительно весов `weights` и `bias`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L0KqxPJU9kAN"
   },
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "  def __init__(self, n_inputs):\n",
    "    # <создать атрибуты объекта weights и bias>\n",
    "    pass\n",
    "  \n",
    "  def forward(self, inputs):\n",
    "    return # <реализовать логику нейрона>\n",
    "  \n",
    "  def backward(self, dvalue):\n",
    "    # dvalue - значение производной, которое приходит нейрону от следующего слоя сети\n",
    "    # в данном случае это будет значение df/dc (созданное методом backwards у объекта MSELoss)\n",
    "    self.dweights = # df/dW\n",
    "    self.dinput =  # df/wX\n",
    "    self.dbias = # df/db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKcO4zOLACxM"
   },
   "source": [
    "4.1.3 Допишите цикл для настройки весов нейрона\n",
    "\n",
    "[SGD](https://ru.wikipedia.org/wiki/%D0%A1%D1%82%D0%BE%D1%85%D0%B0%D1%81%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B9_%D0%B3%D1%80%D0%B0%D0%B4%D0%B8%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B9_%D1%81%D0%BF%D1%83%D1%81%D0%BA)\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/dda3670f8a8996a0d3bf80856bb4a166cc8db6d4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_g_FvwvmALJd"
   },
   "outputs": [],
   "source": [
    "n_inputs = # <размерность элемента выборки >\n",
    "learning_rate = 0.1 #  скорость обучения\n",
    "n_epoch = 100 #  количество эпох\n",
    "\n",
    "neuron = Neuron(n_inputs)\n",
    "loss = MSELoss()\n",
    "\n",
    "losses = []\n",
    "for epoch in range(100):\n",
    "  for x_example, y_example in zip(X, y):\n",
    "    # forward pass\n",
    "    y_pred = # <прогон через нейрон>\n",
    "    curr_loss = # <прогон через функцию потерь>\n",
    "    losses.append(curr_loss)\n",
    "\n",
    "    # backprop\n",
    "    # <вызов методов backward>\n",
    "    # обратите внимание на последовательность вызовов: от конца к началу\n",
    "\n",
    "    # <шаг оптимизации для весов (weights и bias) нейрона>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ebibge9VEgF7"
   },
   "source": [
    "4.2 Решите задачу 2.4.1, используя пакетный градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "as-QeWSdOELd"
   },
   "source": [
    "Вычисления для этой задачи: \n",
    "[1](https://i.ibb.co/rmtQT6P/photo-2021-02-15-18-00-43.jpg)\n",
    "[2](https://i.ibb.co/NmCFVnQ/photo-2021-02-15-18-01-17.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dr9qq4H_J3zt"
   },
   "source": [
    "4.2.1 Модифицируйте класс `MSELoss` из __3.1__, реализовав расчет производной относительно предыдущего слоя с учетом того, что теперь работа ведется с батчами, а не с индивидуальными примерами\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L8wjk9iPMQ4x"
   },
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "  def forward(self, y_pred, y_true):\n",
    "    return # <реализовать логику MSE>\n",
    "\n",
    "  def backward(self, y_pred, y_true):\n",
    "    self.dinput = # df/dy^\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3fSHCEtJjX8"
   },
   "source": [
    "4.2.2. Модифицируйте класс `Neuron` из __4.1.2__:\n",
    "\n",
    "  1) Реализуйте метод `forward` таким образом, чтобы он мог принимать на вход матрицу (батч) с данными. \n",
    "\n",
    "  2) Реализуйте расчет градиента относительно весов `weights` и `bias` с учетом того, что теперь работа ведется с батчами, а не с индивидуальными примерами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_OpuAP0Jpz1"
   },
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "  def __init__(self, n_inputs):\n",
    "    # <создать атрибуты объекта weights и bias>\n",
    "    pass\n",
    "  \n",
    "  def forward(self, inputs):\n",
    "    return # <реализовать логику нейрона>\n",
    "  \n",
    "  def backward(self, dvalue):\n",
    "    # dvalue - значение градиента, которое приходит нейрону от следующего слоя сети\n",
    "    # в данном случае это будет градиент L по y^ (созданный методом backwards у объекта MSELoss)\n",
    "    self.dweights = # df/dW\n",
    "    self.dbias = # df/db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zO-NZrgKMBFx"
   },
   "source": [
    "4.2.3 Допишите цикл для настройки весов нейрона"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zqwm_7eqJim1"
   },
   "outputs": [],
   "source": [
    "n_inputs = # <размерность элемента выборки >\n",
    "learning_rate = 0.1 #  скорость обучения\n",
    "n_epoch = 100 #  количество эпох\n",
    "\n",
    "neuron = Neuron(n_inputs)\n",
    "loss = MSELoss()\n",
    "\n",
    "\n",
    "for epoch in range(100):\n",
    "    # forward pass\n",
    "    y_pred = # <прогон через нейрон>\n",
    "    curr_loss = # <прогон через функцию потерь>\n",
    "    losses.append(curr_loss)\n",
    "\n",
    "    # backprop\n",
    "    # <вызов методов backward>\n",
    "    # обратите внимание на последовательность вызовов: от конца к началу\n",
    "\n",
    "    # <шаг оптимизации для весов (weights и bias) нейрона>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16VtP159OdMk"
   },
   "source": [
    "4.3  Используя один полносвязный слой и  пакетный градиетный спуск, решите задачу регрессии из __2.4.1__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uj5febreSSZ7"
   },
   "source": [
    "4.3.1 Модифицируйте класс `Linear` из __1.4__. ([вычисление градиентов](https://i.ibb.co/kgVR6m6/photo-2021-02-15-21-30-28.jpg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9zWuhaLdSB2_"
   },
   "outputs": [],
   "source": [
    "class Linear:\n",
    "  def __init__(self, n_features, n_neurons):\n",
    "    # <создать атрибуты объекта weights и biases>\n",
    "    pass\n",
    "  \n",
    "  def forward(self, inputs):\n",
    "    return # <реализовать логику слоя>\n",
    "\n",
    "  def backward(self, dvalues):\n",
    "    self.dweights = # df/dW\n",
    "    self.dbiases = # df/db\n",
    "    self.dinputs = # df/dX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3w1hT9MS_Lt"
   },
   "source": [
    "4.3.2 Создайте слой с одним нейроном. Используя класс MSELoss из 2.4.2, убедитесь, что модель обучается"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTkJV-F8TVuN"
   },
   "source": [
    "4.4 Используя наработки из 2.4, создайте нейросеть и решите задачу регрессии.\n",
    "\n",
    "Предлагаемая архитектура: \n",
    "1. Полносвязный слой с 10 нейронами\n",
    "2. Активация ReLU\n",
    "3. Полносвязный слой с 1 нейроном"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "axUjpPz-SvS1"
   },
   "outputs": [],
   "source": [
    "X = torch.linspace(-1, 1, 100).view(-1, 1)\n",
    "y = X.pow(2) + 0.2 * torch.rand(X.size()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LXoiNxkpTziV"
   },
   "outputs": [],
   "source": [
    "class Activation_ReLU:\n",
    "  def forward(self, inputs):\n",
    "    self.inputs = inputs\n",
    "    self.output = inputs.clip(min=0)\n",
    "    return self.output\n",
    "  \n",
    "  def backward(self, dvalues):\n",
    "    self.dinputs = dvalues.clone()\n",
    "    self.dinputs[self.inputs <= 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tXhspwW6T44T"
   },
   "outputs": [],
   "source": [
    "# создание компонентов сети\n",
    "# fc1 = \n",
    "# relu1 = \n",
    "# fc2 = \n",
    "\n",
    "loss = MSELoss()\n",
    "lr = 0.02\n",
    "\n",
    "ys = []\n",
    "for epoch in range(2001):\n",
    "  # <forward pass>\n",
    "  # fc1 > relu1 > fc2 > loss\n",
    "\n",
    "  data_loss = # <прогон через функцию потерь>\n",
    "\n",
    "  if epoch % 200 == 0:\n",
    "    print(f'epoch {epoch} mean loss {data_loss}')\n",
    "    ys.append(out)\n",
    "  \n",
    "  # <backprop> \n",
    "  # loss > fc2 > relu1 > fc1\n",
    "\n",
    "  # <шаг оптимизации для fc1>\n",
    "\n",
    "  # <шаг оптимизации для fc2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kpKi0OfoUkwk"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(len(ys), 1, figsize=(10, 40))\n",
    "for ax, y_ in zip(axs, ys):\n",
    "  ax.scatter(X.numpy(), y.numpy(), color = \"orange\")\n",
    "  ax.plot(X.numpy(), y_.numpy(), 'g-', lw=3)\n",
    "  ax.set_xlim(-1.05, 1.5)\n",
    "  ax.set_ylim(-0.25, 1.25)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPDgJRHjuyArfKO8ZT68MsS",
   "name": "02_NN_blocks_backprop_v1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
